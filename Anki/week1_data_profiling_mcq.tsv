<div class="mcq" data-answer="B"><div class="question">Which discovery type in data profiling focuses on verifying schema, data types, and formats?</div><div class="options"><label class="option"><input type="radio" name="q1" value="A"><span class="opt-text">A. Content discovery</span></label><label class="option"><input type="radio" name="q1" value="B"><span class="opt-text">B. Structure discovery</span></label><label class="option"><input type="radio" name="q1" value="C"><span class="opt-text">C. Relationship discovery</span></label><label class="option"><input type="radio" name="q1" value="D"><span class="opt-text">D. Feature engineering</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: Structure discovery verifies schema, data types, and formats before deeper analysis.</div></div>
<div class="mcq" data-answer="C"><div class="question">Content discovery in data profiling is mainly used to:</div><div class="options"><label class="option"><input type="radio" name="q2" value="A"><span class="opt-text">A. Identify dependencies between tables</span></label><label class="option"><input type="radio" name="q2" value="B"><span class="opt-text">B. Validate data types using regex</span></label><label class="option"><input type="radio" name="q2" value="C"><span class="opt-text">C. Examine missing values, anomalies, and distributions</span></label><label class="option"><input type="radio" name="q2" value="D"><span class="opt-text">D. Deploy the model to production</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: Content discovery examines actual values to detect missing values, anomalies, and value distributions.</div></div>
<div class="mcq" data-answer="B"><div class="question">Relationship discovery typically uses which tool to detect linear relationships between numeric columns?</div><div class="options"><label class="option"><input type="radio" name="q3" value="A"><span class="opt-text">A. df.describe()</span></label><label class="option"><input type="radio" name="q3" value="B"><span class="opt-text">B. df.corr()</span></label><label class="option"><input type="radio" name="q3" value="C"><span class="opt-text">C. df.isnull().sum()</span></label><label class="option"><input type="radio" name="q3" value="D"><span class="opt-text">D. df.dropna()</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: Relationship discovery relies on correlation coefficients and correlation matrices, which df.corr() provides.</div></div>
<div class="mcq" data-answer="D"><div class="question">Single-column profiling usually includes all EXCEPT:</div><div class="options"><label class="option"><input type="radio" name="q4" value="A"><span class="opt-text">A. Cardinality (nulls, distinct counts)</span></label><label class="option"><input type="radio" name="q4" value="B"><span class="opt-text">B. Patterns / data types</span></label><label class="option"><input type="radio" name="q4" value="C"><span class="opt-text">C. Value distributions (histograms, quartiles)</span></label><label class="option"><input type="radio" name="q4" value="D"><span class="opt-text">D. Hyperparameter tuning</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: D</div><div class="explanation">Explanation: Single-column profiling covers cardinality, patterns/data types, and value distributions, not model tuning.</div></div>
<div class="mcq" data-answer="B"><div class="question">Cross/multi-column profiling mainly looks at:</div><div class="options"><label class="option"><input type="radio" name="q5" value="A"><span class="opt-text">A. The best neural network architecture</span></label><label class="option"><input type="radio" name="q5" value="B"><span class="opt-text">B. Relationships like correlations and clusters</span></label><label class="option"><input type="radio" name="q5" value="C"><span class="opt-text">C. Converting images to vectors</span></label><label class="option"><input type="radio" name="q5" value="D"><span class="opt-text">D. Video frame extraction</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: Multi-column profiling analyzes relationships such as correlations, clusters, and dependencies across columns.</div></div>
<div class="mcq" data-answer="B"><div class="question">Data cleaning usually happens:</div><div class="options"><label class="option"><input type="radio" name="q6" value="A"><span class="opt-text">A. Before data profiling</span></label><label class="option"><input type="radio" name="q6" value="B"><span class="opt-text">B. After data profiling</span></label><label class="option"><input type="radio" name="q6" value="C"><span class="opt-text">C. After model deployment</span></label><label class="option"><input type="radio" name="q6" value="D"><span class="opt-text">D. Only after feature selection</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: The slides state that data cleaning usually follows data profiling.</div></div>
<div class="mcq" data-answer="C"><div class="question">Which is an example of a data standardization issue?</div><div class="options"><label class="option"><input type="radio" name="q7" value="A"><span class="opt-text">A. Missing values in Age</span></label><label class="option"><input type="radio" name="q7" value="B"><span class="opt-text">B. Outliers in Revenue</span></label><label class="option"><input type="radio" name="q7" value="C"><span class="opt-text">C. &quot;SG&quot; and &quot;Singapore&quot; meaning the same country</span></label><label class="option"><input type="radio" name="q7" value="D"><span class="opt-text">D. Too many columns</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: The example shows SG vs Singapore for the same country, which is a standardization issue.</div></div>
<div class="mcq" data-answer="B"><div class="question">In the customer example, the invalid email format &quot;mike_tan&quot; is mainly a:</div><div class="options"><label class="option"><input type="radio" name="q8" value="A"><span class="opt-text">A. Schema mismatch</span></label><label class="option"><input type="radio" name="q8" value="B"><span class="opt-text">B. Data quality issue</span></label><label class="option"><input type="radio" name="q8" value="C"><span class="opt-text">C. Feature engineering step</span></label><label class="option"><input type="radio" name="q8" value="D"><span class="opt-text">D. Normalization result</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: The profiling example flags invalid emails like mike_tan as data quality issues.</div></div>
<div class="mcq" data-answer="A"><div class="question">Which pandas function can help detect a low outlier like Age = -5 quickly?</div><div class="options"><label class="option"><input type="radio" name="q9" value="A"><span class="opt-text">A. df[&#x27;Age&#x27;].min()</span></label><label class="option"><input type="radio" name="q9" value="B"><span class="opt-text">B. df[&#x27;Age&#x27;].mode()</span></label><label class="option"><input type="radio" name="q9" value="C"><span class="opt-text">C. df[&#x27;Age&#x27;].max()</span></label><label class="option"><input type="radio" name="q9" value="D"><span class="opt-text">D. df[&#x27;Age&#x27;].corr()</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: A</div><div class="explanation">Explanation: The content discovery example uses df[&#x27;Age&#x27;].min() to spot the -5 low outlier.</div></div>
<div class="mcq" data-answer="C"><div class="question">Which statement best explains the Golden rule for imputation in ML?</div><div class="options"><label class="option"><input type="radio" name="q10" value="A"><span class="opt-text">A. Always impute missing values with mean</span></label><label class="option"><input type="radio" name="q10" value="B"><span class="opt-text">B. Compute imputation values on the full dataset for stability</span></label><label class="option"><input type="radio" name="q10" value="C"><span class="opt-text">C. Fit imputer only on training set to prevent data leakage</span></label><label class="option"><input type="radio" name="q10" value="D"><span class="opt-text">D. Always delete missing rows</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: The golden rule says never compute imputation values before the split; fit on training only to avoid leakage.</div></div>
<div class="mcq" data-answer="B"><div class="question">Filling missing values BEFORE train/test split can cause:</div><div class="options"><label class="option"><input type="radio" name="q11" value="A"><span class="opt-text">A. Underfitting</span></label><label class="option"><input type="radio" name="q11" value="B"><span class="opt-text">B. Data leakage and overly optimistic results</span></label><label class="option"><input type="radio" name="q11" value="C"><span class="opt-text">C. Class imbalance</span></label><label class="option"><input type="radio" name="q11" value="D"><span class="opt-text">D. GPU memory overflow</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: Using all data for imputation leaks test information into training, leading to optimistic results.</div></div>
<div class="mcq" data-answer="C"><div class="question">Forward fill / backward fill is MOST suitable for:</div><div class="options"><label class="option"><input type="radio" name="q12" value="A"><span class="opt-text">A. Image datasets</span></label><label class="option"><input type="radio" name="q12" value="B"><span class="opt-text">B. Text sentiment datasets</span></label><label class="option"><input type="radio" name="q12" value="C"><span class="opt-text">C. Time series data</span></label><label class="option"><input type="radio" name="q12" value="D"><span class="opt-text">D. Network graphs</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: Forward/backward fill is recommended for time series because it preserves temporal continuity.</div></div>
<div class="mcq" data-answer="C"><div class="question">MCAR (Missing Completely at Random) means:</div><div class="options"><label class="option"><input type="radio" name="q13" value="A"><span class="opt-text">A. Missingness depends on the missing value itself</span></label><label class="option"><input type="radio" name="q13" value="B"><span class="opt-text">B. Missingness depends on other observed features</span></label><label class="option"><input type="radio" name="q13" value="C"><span class="opt-text">C. Missingness has no relationship with other data</span></label><label class="option"><input type="radio" name="q13" value="D"><span class="opt-text">D. Missingness only occurs in categorical columns</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: MCAR means missingness is unrelated to other data, so simple imputation is generally safe.</div></div>
<div class="mcq" data-answer="B"><div class="question">For MAR (Missing at Random), a good strategy is to:</div><div class="options"><label class="option"><input type="radio" name="q14" value="A"><span class="opt-text">A. Always delete the column</span></label><label class="option"><input type="radio" name="q14" value="B"><span class="opt-text">B. Impute based on correlated columns</span></label><label class="option"><input type="radio" name="q14" value="C"><span class="opt-text">C. Treat all missing values as outliers</span></label><label class="option"><input type="radio" name="q14" value="D"><span class="opt-text">D. Replace with zero only</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: For MAR, missingness depends on observed features, so imputing using correlated columns is appropriate.</div></div>
<div class="mcq" data-answer="C"><div class="question">For MNAR (Missing Not at Random), the safest approach is usually:</div><div class="options"><label class="option"><input type="radio" name="q15" value="A"><span class="opt-text">A. Simple mean imputation</span></label><label class="option"><input type="radio" name="q15" value="B"><span class="opt-text">B. Ignore the missingness</span></label><label class="option"><input type="radio" name="q15" value="C"><span class="opt-text">C. Avoid simple imputation; model separately or add missing flag</span></label><label class="option"><input type="radio" name="q15" value="D"><span class="opt-text">D. Always forward fill</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: For MNAR, missingness depends on the value itself, so avoid simple imputation and consider a missing flag.</div></div>
<div class="mcq" data-answer="A"><div class="question">Which missing value method is simple but may lose important data when missingness is high?</div><div class="options"><label class="option"><input type="radio" name="q16" value="A"><span class="opt-text">A. Deletion</span></label><label class="option"><input type="radio" name="q16" value="B"><span class="opt-text">B. KNN imputation</span></label><label class="option"><input type="radio" name="q16" value="C"><span class="opt-text">C. Multiple imputation (MICE)</span></label><label class="option"><input type="radio" name="q16" value="D"><span class="opt-text">D. Domain-based imputation</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: A</div><div class="explanation">Explanation: Deletion drops rows/columns and is simple, but can lose a lot of data when missingness is high.</div></div>
<div class="mcq" data-answer="B"><div class="question">Constant value imputation (e.g., &quot;Unknown&quot;) is MOST suitable for:</div><div class="options"><label class="option"><input type="radio" name="q17" value="A"><span class="opt-text">A. Continuous numerical features only</span></label><label class="option"><input type="radio" name="q17" value="B"><span class="opt-text">B. Categorical features</span></label><label class="option"><input type="radio" name="q17" value="C"><span class="opt-text">C. Gaussian-only datasets</span></label><label class="option"><input type="radio" name="q17" value="D"><span class="opt-text">D. Outlier detection</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: The slides recommend constant value imputation like &quot;Unknown&quot; mainly for categorical features.</div></div>
<div class="mcq" data-answer="B"><div class="question">A common disadvantage of mean/median imputation is that it:</div><div class="options"><label class="option"><input type="radio" name="q18" value="A"><span class="opt-text">A. Increases variance</span></label><label class="option"><input type="radio" name="q18" value="B"><span class="opt-text">B. Reduces variance and may ignore relationships</span></label><label class="option"><input type="radio" name="q18" value="C"><span class="opt-text">C. Always creates outliers</span></label><label class="option"><input type="radio" name="q18" value="D"><span class="opt-text">D. Requires GPU acceleration</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: Mean/median imputation can reduce variance and ignore relationships between features.</div></div>
<div class="mcq" data-answer="B"><div class="question">KNN imputation is often:</div><div class="options"><label class="option"><input type="radio" name="q19" value="A"><span class="opt-text">A. Faster than mean imputation on large datasets</span></label><label class="option"><input type="radio" name="q19" value="B"><span class="opt-text">B. More accurate but computationally heavy</span></label><label class="option"><input type="radio" name="q19" value="C"><span class="opt-text">C. Only for text data</span></label><label class="option"><input type="radio" name="q19" value="D"><span class="opt-text">D. Impossible to use in sklearn</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: KNN imputation can be more accurate but is computationally heavy compared to simple methods.</div></div>
<div class="mcq" data-answer="B"><div class="question">Which outlier method works BEST when data is approximately normal (Gaussian)?</div><div class="options"><label class="option"><input type="radio" name="q20" value="A"><span class="opt-text">A. IQR</span></label><label class="option"><input type="radio" name="q20" value="B"><span class="opt-text">B. Z-score</span></label><label class="option"><input type="radio" name="q20" value="C"><span class="opt-text">C. DBSCAN</span></label><label class="option"><input type="radio" name="q20" value="D"><span class="opt-text">D. LOF</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: The slides note Z-score is best for normally distributed data and uses mean/std deviation.</div></div>
<div class="mcq" data-answer="C"><div class="question">In Z-score outlier detection, a common threshold is:</div><div class="options"><label class="option"><input type="radio" name="q21" value="A"><span class="opt-text">A. |Z| &gt; 0.5</span></label><label class="option"><input type="radio" name="q21" value="B"><span class="opt-text">B. |Z| &gt; 1</span></label><label class="option"><input type="radio" name="q21" value="C"><span class="opt-text">C. |Z| &gt; 3</span></label><label class="option"><input type="radio" name="q21" value="D"><span class="opt-text">D. |Z| &gt; 10</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: The outlier slide uses |Z| &gt; 3 as the typical threshold for Z-score detection.</div></div>
<div class="mcq" data-answer="B"><div class="question">Why is Z-score NOT reliable for skewed/non-normal data?</div><div class="options"><label class="option"><input type="radio" name="q22" value="A"><span class="opt-text">A. It cannot be computed without pandas</span></label><label class="option"><input type="radio" name="q22" value="B"><span class="opt-text">B. It is sensitive to extreme values and assumes symmetry</span></label><label class="option"><input type="radio" name="q22" value="C"><span class="opt-text">C. It only works for categorical features</span></label><label class="option"><input type="radio" name="q22" value="D"><span class="opt-text">D. It always flags too many outliers</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: Z-score assumes symmetry and is sensitive to extreme values, so it is unreliable for skewed data.</div></div>
<div class="mcq" data-answer="B"><div class="question">IQR outlier detection is considered robust because it:</div><div class="options"><label class="option"><input type="radio" name="q23" value="A"><span class="opt-text">A. Uses only the maximum value</span></label><label class="option"><input type="radio" name="q23" value="B"><span class="opt-text">B. Uses the middle 50% of data, less affected by extremes</span></label><label class="option"><input type="radio" name="q23" value="C"><span class="opt-text">C. Requires deep learning models</span></label><label class="option"><input type="radio" name="q23" value="D"><span class="opt-text">D. Assumes normal distribution</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: IQR relies on the middle 50% of data and is less influenced by extreme values.</div></div>
<div class="mcq" data-answer="B"><div class="question">Given Q1 = 102.5 and Q3 = 109.5, what is the IQR?</div><div class="options"><label class="option"><input type="radio" name="q24" value="A"><span class="opt-text">A. 5.0</span></label><label class="option"><input type="radio" name="q24" value="B"><span class="opt-text">B. 7.0</span></label><label class="option"><input type="radio" name="q24" value="C"><span class="opt-text">C. 12.0</span></label><label class="option"><input type="radio" name="q24" value="D"><span class="opt-text">D. 17.0</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: IQR = Q3 - Q1 = 109.5 - 102.5 = 7.0, as shown in the example.</div></div>
<div class="mcq" data-answer="C"><div class="question">Given Q1 = 102.5 and IQR = 7.0, the lower fence (1.5 IQR rule) is:</div><div class="options"><label class="option"><input type="radio" name="q25" value="A"><span class="opt-text">A. 120.0</span></label><label class="option"><input type="radio" name="q25" value="B"><span class="opt-text">B. 102.5</span></label><label class="option"><input type="radio" name="q25" value="C"><span class="opt-text">C. 92.0</span></label><label class="option"><input type="radio" name="q25" value="D"><span class="opt-text">D. 85.0</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: Lower fence = Q1 - 1.5 * IQR = 102.5 - 10.5 = 92.0.</div></div>
<div class="mcq" data-answer="C"><div class="question">Isolation Forest detects outliers mainly by:</div><div class="options"><label class="option"><input type="radio" name="q26" value="A"><span class="opt-text">A. Checking if Z-score &gt; 3</span></label><label class="option"><input type="radio" name="q26" value="B"><span class="opt-text">B. Finding points outside boxplot whiskers</span></label><label class="option"><input type="radio" name="q26" value="C"><span class="opt-text">C. Isolating rare points quickly via random splits (short path length)</span></label><label class="option"><input type="radio" name="q26" value="D"><span class="opt-text">D. Computing Pearson correlation</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: Isolation Forest isolates outliers quickly using random splits; short path length indicates an outlier.</div></div>
<div class="mcq" data-answer="B"><div class="question">A key advantage of Isolation Forest is that it:</div><div class="options"><label class="option"><input type="radio" name="q27" value="A"><span class="opt-text">A. Requires perfectly normal data</span></label><label class="option"><input type="radio" name="q27" value="B"><span class="opt-text">B. Works well for high-dimensional and nonlinear patterns</span></label><label class="option"><input type="radio" name="q27" value="C"><span class="opt-text">C. Always gives an interpretable equation</span></label><label class="option"><input type="radio" name="q27" value="D"><span class="opt-text">D. Cannot scale to large datasets</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: The slides list high-dimensional data and nonlinear patterns as strengths of Isolation Forest.</div></div>
<div class="mcq" data-answer="C"><div class="question">A key limitation of Isolation Forest is that it:</div><div class="options"><label class="option"><input type="radio" name="q28" value="A"><span class="opt-text">A. Cannot detect high outliers</span></label><label class="option"><input type="radio" name="q28" value="B"><span class="opt-text">B. Assumes Gaussian distribution</span></label><label class="option"><input type="radio" name="q28" value="C"><span class="opt-text">C. Is harder to explain and needs tuning (contamination rate)</span></label><label class="option"><input type="radio" name="q28" value="D"><span class="opt-text">D. Only works for time series</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: Isolation Forest is harder to explain and has hyperparameters like the contamination rate.</div></div>
<div class="mcq" data-answer="A"><div class="question">LOF (Local Outlier Factor) is mainly based on:</div><div class="options"><label class="option"><input type="radio" name="q29" value="A"><span class="opt-text">A. Density comparison with neighbors</span></label><label class="option"><input type="radio" name="q29" value="B"><span class="opt-text">B. Correlation matrices</span></label><label class="option"><input type="radio" name="q29" value="C"><span class="opt-text">C. Sorting by max value</span></label><label class="option"><input type="radio" name="q29" value="D"><span class="opt-text">D. Frequency encoding</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: A</div><div class="explanation">Explanation: LOF compares a point&#x27;s local density with its neighbors; much lower density indicates an outlier.</div></div>
<div class="mcq" data-answer="C"><div class="question">In DBSCAN, points that do not belong to any cluster are labeled as:</div><div class="options"><label class="option"><input type="radio" name="q30" value="A"><span class="opt-text">A. Core points</span></label><label class="option"><input type="radio" name="q30" value="B"><span class="opt-text">B. Border points</span></label><label class="option"><input type="radio" name="q30" value="C"><span class="opt-text">C. Noise (outliers)</span></label><label class="option"><input type="radio" name="q30" value="D"><span class="opt-text">D. Centroids</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: C</div><div class="explanation">Explanation: DBSCAN labels points not in any cluster as noise, which are treated as outliers.</div></div>
<div class="mcq" data-answer="B"><div class="question">One-Class SVM (OCSVM) detects outliers by:</div><div class="options"><label class="option"><input type="radio" name="q31" value="A"><span class="opt-text">A. Building a correlation matrix</span></label><label class="option"><input type="radio" name="q31" value="B"><span class="opt-text">B. Drawing a boundary in high-dimensional space; outside points are outliers</span></label><label class="option"><input type="radio" name="q31" value="C"><span class="opt-text">C. Using quartiles only</span></label><label class="option"><input type="radio" name="q31" value="D"><span class="opt-text">D. Using mean imputation</span></label></div><div class="feedback" aria-live="polite"></div></div>	<div class="back"><div class="answer">Answer: B</div><div class="explanation">Explanation: OCSVM learns a boundary in high-dimensional space; points outside it are classified as outliers.</div></div>