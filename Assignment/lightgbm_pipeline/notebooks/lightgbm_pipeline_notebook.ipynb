{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk - LightGBM Pipeline\n",
    "\n",
    "This notebook implements a complete ML pipeline with **two modes**:\n",
    "- **TRAIN_MODE = True**: Full training + model saving\n",
    "- **TRAIN_MODE = False**: Load saved models for inference only\n",
    "\n",
    "## Pipeline Stages\n",
    "1. **Data Loading** - Load 7 CSV tables\n",
    "2. **Preprocessing** - Encoding, outlier handling, missing value marking\n",
    "3. **Feature Engineering** - Derived features, aggregations, multi-table joins\n",
    "4. **Model Training/Loading** - K-Fold CV with LightGBM or load saved models\n",
    "5. **Output** - Predictions, feature importance, ROC curves, and training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 1: Configuration & Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Change these settings as needed\n",
    "# =============================================================================\n",
    "TRAIN_MODE = False  # True: train new models, False: load saved models for inference\n",
    "DEBUG = False       # True: use 10000 rows for quick testing\n",
    "NUM_ROWS = 10000 if DEBUG else None\n",
    "\n",
    "DATA_PATH = './home-credit-default-risk/'\n",
    "MODEL_DIR = './saved_models/'  # Directory for saved models\n",
    "SUBMISSION_FILE = 'submission_kernel02.csv'\n",
    "\n",
    "# Create model directory if needed\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Mode: {'TRAINING' if TRAIN_MODE else 'INFERENCE (using saved models)'}\")\n",
    "print(f\"Debug: {DEBUG}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"{title} - done in {time.time() - t0:.0f}s\")\n",
    "\n",
    "# Load all datasets\n",
    "with timer(\"Loading all datasets\"):\n",
    "    application_train = pd.read_csv(f'{DATA_PATH}application_train.csv', nrows=NUM_ROWS)\n",
    "    application_test = pd.read_csv(f'{DATA_PATH}application_test.csv', nrows=NUM_ROWS)\n",
    "    bureau = pd.read_csv(f'{DATA_PATH}bureau.csv', nrows=NUM_ROWS)\n",
    "    bureau_balance = pd.read_csv(f'{DATA_PATH}bureau_balance.csv', nrows=NUM_ROWS)\n",
    "    previous_application = pd.read_csv(f'{DATA_PATH}previous_application.csv', nrows=NUM_ROWS)\n",
    "    pos_cash_balance = pd.read_csv(f'{DATA_PATH}POS_CASH_balance.csv', nrows=NUM_ROWS)\n",
    "    installments_payments = pd.read_csv(f'{DATA_PATH}installments_payments.csv', nrows=NUM_ROWS)\n",
    "    credit_card_balance = pd.read_csv(f'{DATA_PATH}credit_card_balance.csv', nrows=NUM_ROWS)\n",
    "\n",
    "print(f\"Train samples: {len(application_train)}, Test samples: {len(application_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 2: Data Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "with timer(\"Preprocessing application data\"):\n",
    "    df = pd.concat([application_train, application_test], ignore_index=True)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], _ = pd.factorize(df[bin_feature])\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "with timer(\"Preprocessing bureau and bureau_balance\"):\n",
    "    bureau_balance, bb_cat = one_hot_encoder(bureau_balance, nan_as_category=True)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category=True)\n",
    "\n",
    "with timer(\"Preprocessing previous applications\"):\n",
    "    previous_application, prev_cat = one_hot_encoder(previous_application, nan_as_category=True)\n",
    "    for col in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', \n",
    "                'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n",
    "        if col in previous_application.columns:\n",
    "            previous_application[col].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "with timer(\"Preprocessing other tables\"):\n",
    "    pos_cash_balance, pos_cat = one_hot_encoder(pos_cash_balance, nan_as_category=True)\n",
    "    installments_payments, ins_cat = one_hot_encoder(installments_payments, nan_as_category=True)\n",
    "    credit_card_balance, cc_cat = one_hot_encoder(credit_card_balance, nan_as_category=True)\n",
    "\n",
    "del application_train, application_test\n",
    "gc.collect()\n",
    "print(f\"Preprocessed main df shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 3: Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "with timer(\"Creating application features\"):\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "with timer(\"Creating bureau features\"):\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        if col in bureau_balance.columns:\n",
    "            bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    del bureau_balance, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    num_aggregations = {}\n",
    "    agg_mapping = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'], 'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'], 'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'], 'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'], 'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'], 'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'], 'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'], 'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    for col, aggs in agg_mapping.items():\n",
    "        if col in bureau.columns:\n",
    "            num_aggregations[col] = aggs\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        if cat in bureau.columns:\n",
    "            cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat:\n",
    "        col_name = cat + \"_MEAN\"\n",
    "        if col_name in bureau.columns:\n",
    "            cat_aggregations[col_name] = ['mean']\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    if 'CREDIT_ACTIVE_Active' in bureau.columns:\n",
    "        active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "        if len(active) > 0:\n",
    "            active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "            active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "            bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "            del active_agg\n",
    "        del active\n",
    "    if 'CREDIT_ACTIVE_Closed' in bureau.columns:\n",
    "        closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "        if len(closed) > 0:\n",
    "            closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "            closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "            bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "            del closed_agg\n",
    "        del closed\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "    df = df.join(bureau_agg, how='left', on='SK_ID_CURR')\n",
    "    del bureau_agg\n",
    "    gc.collect()\n",
    "    print(f\"After bureau features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating previous application features\"):\n",
    "    previous_application['APP_CREDIT_PERC'] = previous_application['AMT_APPLICATION'] / previous_application['AMT_CREDIT']\n",
    "    prev_num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'], 'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'], 'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'], 'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'], 'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'], 'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    prev_num_aggregations = {k: v for k, v in prev_num_aggregations.items() if k in previous_application.columns}\n",
    "    cat_aggregations = {cat: ['mean'] for cat in prev_cat if cat in previous_application.columns}\n",
    "    prev_agg = previous_application.groupby('SK_ID_CURR').agg({**prev_num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    if 'NAME_CONTRACT_STATUS_Approved' in previous_application.columns:\n",
    "        approved = previous_application[previous_application['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "        if len(approved) > 0:\n",
    "            approved_agg = approved.groupby('SK_ID_CURR').agg(prev_num_aggregations)\n",
    "            approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "            prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "            del approved_agg\n",
    "        del approved\n",
    "    if 'NAME_CONTRACT_STATUS_Refused' in previous_application.columns:\n",
    "        refused = previous_application[previous_application['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "        if len(refused) > 0:\n",
    "            refused_agg = refused.groupby('SK_ID_CURR').agg(prev_num_aggregations)\n",
    "            refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "            prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "            del refused_agg\n",
    "        del refused\n",
    "    del previous_application\n",
    "    gc.collect()\n",
    "    df = df.join(prev_agg, how='left', on='SK_ID_CURR')\n",
    "    del prev_agg\n",
    "    gc.collect()\n",
    "    print(f\"After previous application features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating POS cash features\"):\n",
    "    aggregations = {'MONTHS_BALANCE': ['max', 'mean', 'size'], 'SK_DPD': ['max', 'mean'], 'SK_DPD_DEF': ['max', 'mean']}\n",
    "    aggregations = {k: v for k, v in aggregations.items() if k in pos_cash_balance.columns}\n",
    "    for cat in pos_cat:\n",
    "        if cat in pos_cash_balance.columns:\n",
    "            aggregations[cat] = ['mean']\n",
    "    pos_agg = pos_cash_balance.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    pos_agg['POS_COUNT'] = pos_cash_balance.groupby('SK_ID_CURR').size()\n",
    "    del pos_cash_balance\n",
    "    gc.collect()\n",
    "    df = df.join(pos_agg, how='left', on='SK_ID_CURR')\n",
    "    del pos_agg\n",
    "    gc.collect()\n",
    "    print(f\"After POS cash features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating installments features\"):\n",
    "    installments_payments['PAYMENT_PERC'] = installments_payments['AMT_PAYMENT'] / installments_payments['AMT_INSTALMENT']\n",
    "    installments_payments['PAYMENT_DIFF'] = installments_payments['AMT_INSTALMENT'] - installments_payments['AMT_PAYMENT']\n",
    "    installments_payments['DPD'] = installments_payments['DAYS_ENTRY_PAYMENT'] - installments_payments['DAYS_INSTALMENT']\n",
    "    installments_payments['DBD'] = installments_payments['DAYS_INSTALMENT'] - installments_payments['DAYS_ENTRY_PAYMENT']\n",
    "    installments_payments['DPD'] = installments_payments['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    installments_payments['DBD'] = installments_payments['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins_aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'], 'DPD': ['max', 'mean', 'sum'], 'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'], 'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'], 'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    ins_aggregations = {k: v for k, v in ins_aggregations.items() if k in installments_payments.columns}\n",
    "    for cat in ins_cat:\n",
    "        if cat in installments_payments.columns:\n",
    "            ins_aggregations[cat] = ['mean']\n",
    "    ins_agg = installments_payments.groupby('SK_ID_CURR').agg(ins_aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    ins_agg['INSTAL_COUNT'] = installments_payments.groupby('SK_ID_CURR').size()\n",
    "    del installments_payments\n",
    "    gc.collect()\n",
    "    df = df.join(ins_agg, how='left', on='SK_ID_CURR')\n",
    "    del ins_agg\n",
    "    gc.collect()\n",
    "    print(f\"After installments features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating credit card features\"):\n",
    "    if 'SK_ID_PREV' in credit_card_balance.columns:\n",
    "        credit_card_balance.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    numeric_cols = credit_card_balance.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != 'SK_ID_CURR']\n",
    "    cc_agg = credit_card_balance.groupby('SK_ID_CURR')[numeric_cols].agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    cc_agg['CC_COUNT'] = credit_card_balance.groupby('SK_ID_CURR').size()\n",
    "    del credit_card_balance\n",
    "    gc.collect()\n",
    "    df = df.join(cc_agg, how='left', on='SK_ID_CURR')\n",
    "    del cc_agg\n",
    "    gc.collect()\n",
    "\n",
    "df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "print(f\"Final df shape after all features: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 4: Model Training OR Loading\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Split data\n",
    "train_df = df[df['TARGET'].notnull()].copy()\n",
    "test_df = df[df['TARGET'].isnull()].copy()\n",
    "feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']]\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "print(f\"Number of features: {len(feats)}\")\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "num_folds = 3 if DEBUG else 10\n",
    "\n",
    "if TRAIN_MODE:\n",
    "    # ==================== TRAINING MODE ====================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING MODE: Training new models\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    training_history = []\n",
    "    fold_results = []\n",
    "    models = []  # Store trained models\n",
    "    \n",
    "    with timer(f\"Training {num_folds}-Fold LightGBM\"):\n",
    "        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "            train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "            valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "            \n",
    "            clf = LGBMClassifier(\n",
    "                n_jobs=4, n_estimators=10000, learning_rate=0.02, num_leaves=34,\n",
    "                colsample_bytree=0.9497036, subsample=0.8715623, max_depth=8,\n",
    "                reg_alpha=0.041545473, reg_lambda=0.0735294, min_split_gain=0.0222415,\n",
    "                min_child_weight=39.3259775, verbose=-1,\n",
    "            )\n",
    "            \n",
    "            eval_results = {}\n",
    "            clf.fit(\n",
    "                train_x, train_y,\n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_names=['train', 'valid'],\n",
    "                eval_metric='auc',\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=200),\n",
    "                    lgb.log_evaluation(period=200),\n",
    "                    lgb.record_evaluation(eval_results)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(MODEL_DIR, f'lgbm_fold_{n_fold}.pkl')\n",
    "            joblib.dump(clf, model_path)\n",
    "            models.append(clf)\n",
    "            \n",
    "            training_history.append({\n",
    "                'fold': n_fold + 1,\n",
    "                'train_auc': eval_results['train']['auc'],\n",
    "                'valid_auc': eval_results['valid']['auc'],\n",
    "                'best_iteration': clf.best_iteration_\n",
    "            })\n",
    "            \n",
    "            valid_preds = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "            oof_preds[valid_idx] = valid_preds\n",
    "            sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / num_folds\n",
    "            \n",
    "            fold_results.append({\n",
    "                'fold': n_fold + 1,\n",
    "                'y_true': valid_y.values,\n",
    "                'y_pred': valid_preds,\n",
    "                'auc': roc_auc_score(valid_y, valid_preds)\n",
    "            })\n",
    "            \n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "            \n",
    "            print(f'Fold {n_fold + 1:2d} AUC: {fold_results[-1][\"auc\"]:.6f} | Model saved to: {model_path}')\n",
    "            \n",
    "            del train_x, train_y, valid_x, valid_y\n",
    "            gc.collect()\n",
    "    \n",
    "    # Save feature list for inference\n",
    "    joblib.dump(feats, os.path.join(MODEL_DIR, 'feature_list.pkl'))\n",
    "    \n",
    "    full_auc = roc_auc_score(train_df['TARGET'], oof_preds)\n",
    "    print(f'\\nFull OOF AUC: {full_auc:.6f}')\n",
    "    print(f'Models saved to: {MODEL_DIR}')\n",
    "    \n",
    "    train_targets = train_df['TARGET'].values\n",
    "\n",
    "else:\n",
    "    # ==================== INFERENCE MODE ====================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE MODE: Loading saved models\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load feature list\n",
    "    saved_feats = joblib.load(os.path.join(MODEL_DIR, 'feature_list.pkl'))\n",
    "    \n",
    "    # Align features\n",
    "    missing_feats = [f for f in saved_feats if f not in test_df.columns]\n",
    "    if missing_feats:\n",
    "        print(f\"Warning: {len(missing_feats)} features missing, filling with 0\")\n",
    "        for f in missing_feats:\n",
    "            test_df[f] = 0\n",
    "    \n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    models = []\n",
    "    \n",
    "    with timer(\"Loading models and making predictions\"):\n",
    "        for n_fold in range(num_folds):\n",
    "            model_path = os.path.join(MODEL_DIR, f'lgbm_fold_{n_fold}.pkl')\n",
    "            if os.path.exists(model_path):\n",
    "                clf = joblib.load(model_path)\n",
    "                models.append(clf)\n",
    "                sub_preds += clf.predict_proba(test_df[saved_feats])[:, 1] / num_folds\n",
    "                print(f'Loaded model from: {model_path}')\n",
    "            else:\n",
    "                print(f'Warning: Model not found: {model_path}')\n",
    "    \n",
    "    print(f'\\nLoaded {len(models)} models for inference')\n",
    "    \n",
    "    # Placeholders for visualization (not available in inference mode)\n",
    "    oof_preds = None\n",
    "    train_targets = None\n",
    "    training_history = None\n",
    "    fold_results = None\n",
    "    feature_importance_df = None\n",
    "\n",
    "# Store predictions\n",
    "test_df['TARGET'] = sub_preds\n",
    "submission_df = test_df[['SK_ID_CURR', 'TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 5: Output & Visualization\n",
    "# =============================================================================\n",
    "\n",
    "# Save submission\n",
    "with timer(\"Saving submission file\"):\n",
    "    submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "    print(f\"Submission saved to: {SUBMISSION_FILE}\")\n",
    "    print(f\"Shape: {submission_df.shape}\")\n",
    "    print(f\"Predictions range: [{submission_df['TARGET'].min():.4f}, {submission_df['TARGET'].max():.4f}]\")\n",
    "\n",
    "# Visualizations only available in TRAIN_MODE\n",
    "if TRAIN_MODE and feature_importance_df is not None:\n",
    "    \n",
    "    # Feature Importance\n",
    "    with timer(\"Generating feature importance\"):\n",
    "        cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "                .groupby(\"feature\").mean()\n",
    "                .sort_values(by=\"importance\", ascending=False)[:40].index)\n",
    "        best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "        plt.figure(figsize=(10, 12))\n",
    "        sns.barplot(x=\"importance\", y=\"feature\", \n",
    "                    data=best_features.sort_values(by=\"importance\", ascending=False), palette=\"viridis\")\n",
    "        plt.title('LightGBM Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('lgbm_importances01.png', dpi=150)\n",
    "        plt.show()\n",
    "    \n",
    "    # Per-fold ROC curves\n",
    "    with timer(\"Generating per-fold ROC curves\"):\n",
    "        n_folds = len(fold_results)\n",
    "        cols = min(5, n_folds)\n",
    "        rows = (n_folds + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "        axes = axes.flatten() if n_folds > 1 else [axes]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 0.9, n_folds))\n",
    "        for idx, fold_data in enumerate(fold_results):\n",
    "            ax = axes[idx]\n",
    "            fpr, tpr, _ = roc_curve(fold_data['y_true'], fold_data['y_pred'])\n",
    "            ax.plot(fpr, tpr, color=colors[idx], lw=2, label=f'AUC = {fold_data[\"auc\"]:.4f}')\n",
    "            ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "            ax.set_title(f'Fold {fold_data[\"fold\"]}')\n",
    "            ax.legend(loc='lower right')\n",
    "            ax.grid(alpha=0.3)\n",
    "        for idx in range(n_folds, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        plt.suptitle('ROC Curves for Each Fold', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curves_per_fold.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Combined ROC\n",
    "    with timer(\"Generating combined ROC curve\"):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for fold_data in fold_results:\n",
    "            fpr, tpr, _ = roc_curve(fold_data['y_true'], fold_data['y_pred'])\n",
    "            plt.plot(fpr, tpr, alpha=0.3, lw=1)\n",
    "        fpr, tpr, _ = roc_curve(train_targets, oof_preds)\n",
    "        oof_auc = roc_auc_score(train_targets, oof_preds)\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Mean OOF (AUC = {oof_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'navy', lw=2, linestyle='--')\n",
    "        plt.title('Combined ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curve.png', dpi=150)\n",
    "        plt.show()\n",
    "    \n",
    "    # Training curves\n",
    "    with timer(\"Generating training curves\"):\n",
    "        n_folds = len(training_history)\n",
    "        cols = min(5, n_folds)\n",
    "        rows = (n_folds + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n",
    "        axes = axes.flatten() if n_folds > 1 else [axes]\n",
    "        for idx, fold_history in enumerate(training_history):\n",
    "            ax = axes[idx]\n",
    "            iterations = range(1, len(fold_history['train_auc']) + 1)\n",
    "            ax.plot(iterations, fold_history['train_auc'], label='Train', color='blue', alpha=0.7)\n",
    "            ax.plot(iterations, fold_history['valid_auc'], label='Valid', color='red', alpha=0.7)\n",
    "            ax.axvline(x=fold_history['best_iteration'], color='green', linestyle='--', alpha=0.7)\n",
    "            ax.set_title(f'Fold {fold_history[\"fold\"]} (best: {fold_history[\"best_iteration\"]})')\n",
    "            ax.legend(loc='lower right', fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "        for idx in range(n_folds, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        plt.suptitle('Training Curves', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for fr in fold_results:\n",
    "        print(f\"  Fold {fr['fold']:2d}: AUC = {fr['auc']:.6f}\")\n",
    "    print(f\"\\n  Mean Fold AUC: {np.mean([fr['auc'] for fr in fold_results]):.6f}\")\n",
    "    print(f\"  OOF AUC:       {oof_auc:.6f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Predictions generated using {len(models)} saved models\")\n",
    "    print(f\"Visualizations skipped (only available in TRAIN_MODE)\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {SUBMISSION_FILE}\")\n",
    "if TRAIN_MODE:\n",
    "    print(f\"  - {MODEL_DIR}lgbm_fold_*.pkl (saved models)\")\n",
    "    print(f\"  - lgbm_importances01.png\")\n",
    "    print(f\"  - roc_curves_per_fold.png\")\n",
    "    print(f\"  - roc_curve.png\")\n",
    "    print(f\"  - training_curves.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
