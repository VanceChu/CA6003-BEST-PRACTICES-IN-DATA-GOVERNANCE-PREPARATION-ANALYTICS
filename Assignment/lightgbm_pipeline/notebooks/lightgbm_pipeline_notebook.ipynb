{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk - LightGBM Pipeline\n",
    "\n",
    "This notebook implements a complete ML pipeline with **two modes**:\n",
    "- **TRAIN_MODE = True**: Full training + model saving\n",
    "- **TRAIN_MODE = False**: Load saved models for inference only\n",
    "\n",
    "## Pipeline Stages\n",
    "1. **Data Loading** - Load 7 CSV tables\n",
    "2. **Preprocessing** - Encoding, outlier handling, missing value marking\n",
    "3. **Feature Engineering** - Derived features, aggregations, multi-table joins\n",
    "4. **Model Training/Loading** - K-Fold CV with LightGBM or load saved models\n",
    "5. **Output** - Predictions, feature importance, ROC curves, and training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Environment Detection and Configuration\n",
    "# =============================================================================\n",
    "import os\n",
    "import platform\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect running environment: local Mac, Kaggle, or other.\"\"\"\n",
    "    if os.path.exists('/kaggle'):\n",
    "        return 'kaggle'\n",
    "    elif platform.system() == 'Darwin':\n",
    "        return 'mac'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "ENVIRONMENT = detect_environment()\n",
    "print(f\"Detected environment: {ENVIRONMENT}\")\n",
    "\n",
    "# Set data paths based on environment\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    DATA_PATH = '/kaggle/input/home-credit-default-risk/'\n",
    "    OUTPUT_PATH = '/kaggle/working/'\n",
    "    MODEL_DIR = '/kaggle/working/saved_models/'\n",
    "    LOG_DIR = '/kaggle/working/logs/'\n",
    "else:  # mac or other (local)\n",
    "    DATA_PATH = './home-credit-default-risk/'\n",
    "    OUTPUT_PATH = './'\n",
    "    MODEL_DIR = './saved_models/'\n",
    "    LOG_DIR = './logs/'\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Log directory: {LOG_DIR}\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPU Detection for LightGBM\n",
    "# =============================================================================\n",
    "import subprocess\n",
    "\n",
    "def detect_gpu():\n",
    "    \"\"\"Detect if NVIDIA GPU is available.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            return True, \"NVIDIA GPU detected\"\n",
    "    except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "        pass\n",
    "    return False, \"No GPU detected\"\n",
    "\n",
    "GPU_AVAILABLE, GPU_INFO = detect_gpu()\n",
    "print(f\"GPU Detection: {GPU_INFO}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Important Note about LightGBM GPU\n",
    "# =============================================================================\n",
    "# LightGBM GPU acceleration requires:\n",
    "# 1. A GPU-enabled build of LightGBM (pip default is CPU-only)\n",
    "# 2. OpenCL or CUDA libraries\n",
    "#\n",
    "# On Kaggle, even with GPU enabled, the pre-installed LightGBM is CPU version.\n",
    "# GPU acceleration is more beneficial for XGBoost or CatBoost on Kaggle.\n",
    "#\n",
    "# For this notebook, we'll use CPU training which is still fast for LightGBM.\n",
    "# =============================================================================\n",
    "\n",
    "# Force CPU mode for reliable execution\n",
    "USE_GPU = False\n",
    "DEVICE_TYPE = 'cpu'\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"\\n⚠️ Note: GPU detected but LightGBM will use CPU.\")\n",
    "    print(\"   Reason: Kaggle's LightGBM is CPU-only build.\")\n",
    "    print(\"   This is normal - LightGBM is already very fast on CPU!\")\n",
    "else:\n",
    "    print(\"\\n→ Training will use CPU\")\n",
    "\n",
    "print(f\"\\nDevice for training: {DEVICE_TYPE.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 1: Configuration & Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "TRAIN_MODE = True   # True: train new models, False: load saved models\n",
    "DEBUG = False       # True: use 10000 rows for quick testing\n",
    "NUM_ROWS = 10000 if DEBUG else None\n",
    "\n",
    "# Paths are set by environment detection cell above\n",
    "SUBMISSION_FILE = os.path.join(OUTPUT_PATH, 'submission_kernel02.csv')\n",
    "\n",
    "# =============================================================================\n",
    "# Logging Setup - Detailed epoch logging to file\n",
    "# =============================================================================\n",
    "log_filename = os.path.join(LOG_DIR, f'training_log_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "\n",
    "file_handler = logging.FileHandler(log_filename, encoding='utf-8')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s | %(levelname)s | %(message)s'))\n",
    "\n",
    "train_logger = logging.getLogger('lightgbm_training')\n",
    "train_logger.setLevel(logging.DEBUG)\n",
    "# Clear existing handlers to avoid duplicates\n",
    "train_logger.handlers.clear()\n",
    "train_logger.addHandler(file_handler)\n",
    "\n",
    "print(f\"Mode: {'TRAINING' if TRAIN_MODE else 'INFERENCE'}\")\n",
    "print(f\"Debug: {DEBUG}\")\n",
    "print(f\"Log file: {log_filename}\")\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"{title} - done in {elapsed:.0f}s\")\n",
    "    train_logger.info(f\"{title} completed in {elapsed:.0f}s\")\n",
    "\n",
    "# Load all datasets\n",
    "with timer(\"Loading all datasets\"):\n",
    "    application_train = pd.read_csv(f'{DATA_PATH}application_train.csv', nrows=NUM_ROWS)\n",
    "    application_test = pd.read_csv(f'{DATA_PATH}application_test.csv', nrows=NUM_ROWS)\n",
    "    bureau = pd.read_csv(f'{DATA_PATH}bureau.csv', nrows=NUM_ROWS)\n",
    "    bureau_balance = pd.read_csv(f'{DATA_PATH}bureau_balance.csv', nrows=NUM_ROWS)\n",
    "    previous_application = pd.read_csv(f'{DATA_PATH}previous_application.csv', nrows=NUM_ROWS)\n",
    "    pos_cash_balance = pd.read_csv(f'{DATA_PATH}POS_CASH_balance.csv', nrows=NUM_ROWS)\n",
    "    installments_payments = pd.read_csv(f'{DATA_PATH}installments_payments.csv', nrows=NUM_ROWS)\n",
    "    credit_card_balance = pd.read_csv(f'{DATA_PATH}credit_card_balance.csv', nrows=NUM_ROWS)\n",
    "\n",
    "print(f\"Train samples: {len(application_train)}, Test samples: {len(application_test)}\")\n",
    "train_logger.info(f\"Loaded data - Train: {len(application_train)}, Test: {len(application_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 2: Data Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "with timer(\"Preprocessing application data\"):\n",
    "    df = pd.concat([application_train, application_test], ignore_index=True)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], _ = pd.factorize(df[bin_feature])\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "with timer(\"Preprocessing bureau and bureau_balance\"):\n",
    "    bureau_balance, bb_cat = one_hot_encoder(bureau_balance, nan_as_category=True)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category=True)\n",
    "\n",
    "with timer(\"Preprocessing previous applications\"):\n",
    "    previous_application, prev_cat = one_hot_encoder(previous_application, nan_as_category=True)\n",
    "    for col in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', \n",
    "                'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n",
    "        if col in previous_application.columns:\n",
    "            previous_application[col].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "with timer(\"Preprocessing other tables\"):\n",
    "    pos_cash_balance, pos_cat = one_hot_encoder(pos_cash_balance, nan_as_category=True)\n",
    "    installments_payments, ins_cat = one_hot_encoder(installments_payments, nan_as_category=True)\n",
    "    credit_card_balance, cc_cat = one_hot_encoder(credit_card_balance, nan_as_category=True)\n",
    "\n",
    "del application_train, application_test\n",
    "gc.collect()\n",
    "print(f\"Preprocessed main df shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 3: Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "with timer(\"Creating application features\"):\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "with timer(\"Creating bureau features\"):\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        if col in bureau_balance.columns:\n",
    "            bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    del bureau_balance, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    num_aggregations = {}\n",
    "    agg_mapping = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'], 'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'], 'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'], 'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'], 'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'], 'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'], 'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'], 'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    for col, aggs in agg_mapping.items():\n",
    "        if col in bureau.columns:\n",
    "            num_aggregations[col] = aggs\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        if cat in bureau.columns:\n",
    "            cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat:\n",
    "        col_name = cat + \"_MEAN\"\n",
    "        if col_name in bureau.columns:\n",
    "            cat_aggregations[col_name] = ['mean']\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    if 'CREDIT_ACTIVE_Active' in bureau.columns:\n",
    "        active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "        if len(active) > 0:\n",
    "            active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "            active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "            bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "            del active_agg\n",
    "        del active\n",
    "    if 'CREDIT_ACTIVE_Closed' in bureau.columns:\n",
    "        closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "        if len(closed) > 0:\n",
    "            closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "            closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "            bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "            del closed_agg\n",
    "        del closed\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "    df = df.join(bureau_agg, how='left', on='SK_ID_CURR')\n",
    "    del bureau_agg\n",
    "    gc.collect()\n",
    "    print(f\"After bureau features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating previous application features\"):\n",
    "    previous_application['APP_CREDIT_PERC'] = previous_application['AMT_APPLICATION'] / previous_application['AMT_CREDIT']\n",
    "    prev_num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'], 'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'], 'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'], 'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'], 'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'], 'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    prev_num_aggregations = {k: v for k, v in prev_num_aggregations.items() if k in previous_application.columns}\n",
    "    cat_aggregations = {cat: ['mean'] for cat in prev_cat if cat in previous_application.columns}\n",
    "    prev_agg = previous_application.groupby('SK_ID_CURR').agg({**prev_num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    if 'NAME_CONTRACT_STATUS_Approved' in previous_application.columns:\n",
    "        approved = previous_application[previous_application['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "        if len(approved) > 0:\n",
    "            approved_agg = approved.groupby('SK_ID_CURR').agg(prev_num_aggregations)\n",
    "            approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "            prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "            del approved_agg\n",
    "        del approved\n",
    "    if 'NAME_CONTRACT_STATUS_Refused' in previous_application.columns:\n",
    "        refused = previous_application[previous_application['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "        if len(refused) > 0:\n",
    "            refused_agg = refused.groupby('SK_ID_CURR').agg(prev_num_aggregations)\n",
    "            refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "            prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "            del refused_agg\n",
    "        del refused\n",
    "    del previous_application\n",
    "    gc.collect()\n",
    "    df = df.join(prev_agg, how='left', on='SK_ID_CURR')\n",
    "    del prev_agg\n",
    "    gc.collect()\n",
    "    print(f\"After previous application features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating POS cash features\"):\n",
    "    aggregations = {'MONTHS_BALANCE': ['max', 'mean', 'size'], 'SK_DPD': ['max', 'mean'], 'SK_DPD_DEF': ['max', 'mean']}\n",
    "    aggregations = {k: v for k, v in aggregations.items() if k in pos_cash_balance.columns}\n",
    "    for cat in pos_cat:\n",
    "        if cat in pos_cash_balance.columns:\n",
    "            aggregations[cat] = ['mean']\n",
    "    pos_agg = pos_cash_balance.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    pos_agg['POS_COUNT'] = pos_cash_balance.groupby('SK_ID_CURR').size()\n",
    "    del pos_cash_balance\n",
    "    gc.collect()\n",
    "    df = df.join(pos_agg, how='left', on='SK_ID_CURR')\n",
    "    del pos_agg\n",
    "    gc.collect()\n",
    "    print(f\"After POS cash features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating installments features\"):\n",
    "    installments_payments['PAYMENT_PERC'] = installments_payments['AMT_PAYMENT'] / installments_payments['AMT_INSTALMENT']\n",
    "    installments_payments['PAYMENT_DIFF'] = installments_payments['AMT_INSTALMENT'] - installments_payments['AMT_PAYMENT']\n",
    "    installments_payments['DPD'] = installments_payments['DAYS_ENTRY_PAYMENT'] - installments_payments['DAYS_INSTALMENT']\n",
    "    installments_payments['DBD'] = installments_payments['DAYS_INSTALMENT'] - installments_payments['DAYS_ENTRY_PAYMENT']\n",
    "    installments_payments['DPD'] = installments_payments['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    installments_payments['DBD'] = installments_payments['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins_aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'], 'DPD': ['max', 'mean', 'sum'], 'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'], 'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'], 'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    ins_aggregations = {k: v for k, v in ins_aggregations.items() if k in installments_payments.columns}\n",
    "    for cat in ins_cat:\n",
    "        if cat in installments_payments.columns:\n",
    "            ins_aggregations[cat] = ['mean']\n",
    "    ins_agg = installments_payments.groupby('SK_ID_CURR').agg(ins_aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    ins_agg['INSTAL_COUNT'] = installments_payments.groupby('SK_ID_CURR').size()\n",
    "    del installments_payments\n",
    "    gc.collect()\n",
    "    df = df.join(ins_agg, how='left', on='SK_ID_CURR')\n",
    "    del ins_agg\n",
    "    gc.collect()\n",
    "    print(f\"After installments features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating credit card features\"):\n",
    "    if 'SK_ID_PREV' in credit_card_balance.columns:\n",
    "        credit_card_balance.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    numeric_cols = credit_card_balance.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != 'SK_ID_CURR']\n",
    "    cc_agg = credit_card_balance.groupby('SK_ID_CURR')[numeric_cols].agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    cc_agg['CC_COUNT'] = credit_card_balance.groupby('SK_ID_CURR').size()\n",
    "    del credit_card_balance\n",
    "    gc.collect()\n",
    "    df = df.join(cc_agg, how='left', on='SK_ID_CURR')\n",
    "    del cc_agg\n",
    "    gc.collect()\n",
    "\n",
    "df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "print(f\"Final df shape after all features: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 4: Model Training OR Loading\n",
    "# =============================================================================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# =============================================================================\n",
    "# Custom Callback: Log every epoch to file, print every 50 to console\n",
    "# =============================================================================\n",
    "class TrainingLogger:\n",
    "    \"\"\"Log every epoch to file, print every N epochs to console.\"\"\"\n",
    "    def __init__(self, logger, fold_num, console_period=50):\n",
    "        self.logger = logger\n",
    "        self.fold_num = fold_num\n",
    "        self.console_period = console_period\n",
    "        self.start_time = None\n",
    "    \n",
    "    def __call__(self, env):\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        iteration = env.iteration + 1\n",
    "        train_auc = valid_auc = None\n",
    "        \n",
    "        for data_name, eval_name, result, _ in env.evaluation_result_list:\n",
    "            if data_name == 'train' and eval_name == 'auc':\n",
    "                train_auc = result\n",
    "            elif data_name == 'valid' and eval_name == 'auc':\n",
    "                valid_auc = result\n",
    "        \n",
    "        # Log EVERY epoch to file\n",
    "        elapsed = time.time() - self.start_time\n",
    "        train_logger.debug(f\"Fold {self.fold_num} | Epoch {iteration:5d} | Train AUC: {train_auc:.6f} | Valid AUC: {valid_auc:.6f} | Time: {elapsed:.1f}s\")\n",
    "        \n",
    "        # Print every 50 epochs to console\n",
    "        if iteration % self.console_period == 0:\n",
    "            print(f\"  [Fold {self.fold_num}] Epoch {iteration:5d}: Train={train_auc:.6f}, Valid={valid_auc:.6f}\")\n",
    "\n",
    "# Split data\n",
    "train_df = df[df['TARGET'].notnull()].copy()\n",
    "test_df = df[df['TARGET'].isnull()].copy()\n",
    "feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']]\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "print(f\"Number of features: {len(feats)}\")\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "num_folds = 3 if DEBUG else 10\n",
    "\n",
    "# =============================================================================\n",
    "# LightGBM Parameters - Native API with proper GPU support\n",
    "# =============================================================================\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 34,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.9497036,\n",
    "    'bagging_fraction': 0.8715623,\n",
    "    'bagging_freq': 1,\n",
    "    'max_depth': 8,\n",
    "    'lambda_l1': 0.041545473,\n",
    "    'lambda_l2': 0.0735294,\n",
    "    'min_split_gain': 0.0222415,\n",
    "    'min_child_weight': 39.3259775,\n",
    "    'verbose': -1,\n",
    "    'num_threads': 4,\n",
    "}\n",
    "\n",
    "# Add GPU or CPU specific parameters\n",
    "if USE_GPU:\n",
    "    lgb_params['device_type'] = 'gpu'\n",
    "    lgb_params['gpu_platform_id'] = 0\n",
    "    lgb_params['gpu_device_id'] = 0\n",
    "    lgb_params['num_threads'] = 1  # GPU typically uses single thread for data loading\n",
    "    print(\"\\n✓ GPU acceleration ENABLED (using lgb.train native API)\")\n",
    "    train_logger.info(\"GPU acceleration enabled with native API\")\n",
    "else:\n",
    "    lgb_params['device_type'] = 'cpu'\n",
    "    print(\"\\n→ Training on CPU\")\n",
    "    train_logger.info(\"Training on CPU\")\n",
    "\n",
    "if TRAIN_MODE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING MODE: Training new models\")\n",
    "    print(\"=\"*60)\n",
    "    train_logger.info(\"=\"*60)\n",
    "    train_logger.info(f\"TRAINING MODE | Folds: {num_folds} | Device: {lgb_params['device_type'].upper()}\")\n",
    "    train_logger.info(\"=\"*60)\n",
    "    \n",
    "    folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    training_history = []\n",
    "    fold_results = []\n",
    "    models = []\n",
    "    \n",
    "    with timer(f\"Training {num_folds}-Fold LightGBM\"):\n",
    "        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "            fold_start = time.time()\n",
    "            print(f\"\\n--- Fold {n_fold + 1}/{num_folds} ---\")\n",
    "            train_logger.info(f\"\\n{'='*40}\")\n",
    "            train_logger.info(f\"Fold {n_fold + 1} | Train: {len(train_idx)}, Valid: {len(valid_idx)}\")\n",
    "            \n",
    "            train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "            valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "            \n",
    "            # Create LightGBM Dataset objects (native API)\n",
    "            lgb_train = lgb.Dataset(train_x, label=train_y)\n",
    "            lgb_valid = lgb.Dataset(valid_x, label=valid_y, reference=lgb_train)\n",
    "            \n",
    "            # Custom training logger callback\n",
    "            training_callback = TrainingLogger(train_logger, n_fold + 1, console_period=50)\n",
    "            \n",
    "            # Store evaluation results\n",
    "            evals_result = {}\n",
    "            \n",
    "            # Train using native API\n",
    "            model = lgb.train(\n",
    "                lgb_params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=[lgb_train, lgb_valid],\n",
    "                valid_names=['train', 'valid'],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=200),\n",
    "                    lgb.record_evaluation(evals_result),\n",
    "                    training_callback\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(MODEL_DIR, f'lgbm_fold_{n_fold}.txt')\n",
    "            model.save_model(model_path)\n",
    "            models.append(model)\n",
    "            \n",
    "            training_history.append({\n",
    "                'fold': n_fold + 1,\n",
    "                'train_auc': evals_result['train']['auc'],\n",
    "                'valid_auc': evals_result['valid']['auc'],\n",
    "                'best_iteration': model.best_iteration\n",
    "            })\n",
    "            \n",
    "            # Predictions\n",
    "            valid_preds = model.predict(valid_x, num_iteration=model.best_iteration)\n",
    "            oof_preds[valid_idx] = valid_preds\n",
    "            sub_preds += model.predict(test_df[feats], num_iteration=model.best_iteration) / num_folds\n",
    "            \n",
    "            fold_auc = roc_auc_score(valid_y, valid_preds)\n",
    "            fold_results.append({\n",
    "                'fold': n_fold + 1,\n",
    "                'y_true': valid_y.values,\n",
    "                'y_pred': valid_preds,\n",
    "                'auc': fold_auc\n",
    "            })\n",
    "            \n",
    "            # Feature importance\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = model.feature_importance(importance_type='gain')\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "            \n",
    "            fold_time = time.time() - fold_start\n",
    "            print(f'Fold {n_fold + 1:2d} AUC: {fold_auc:.6f} | Best iter: {model.best_iteration} | Time: {fold_time:.0f}s')\n",
    "            train_logger.info(f\"Fold {n_fold + 1} DONE: AUC={fold_auc:.6f}, Best={model.best_iteration}, Time={fold_time:.0f}s\")\n",
    "            \n",
    "            del train_x, train_y, valid_x, valid_y, lgb_train, lgb_valid\n",
    "            gc.collect()\n",
    "    \n",
    "    # Save feature list for inference\n",
    "    joblib.dump(feats, os.path.join(MODEL_DIR, 'feature_list.pkl'))\n",
    "    \n",
    "    full_auc = roc_auc_score(train_df['TARGET'], oof_preds)\n",
    "    print(f'\\nFull OOF AUC: {full_auc:.6f}')\n",
    "    print(f'Models saved to: {MODEL_DIR}')\n",
    "    train_logger.info(f\"\\nFINAL OOF AUC: {full_auc:.6f}\")\n",
    "    train_logger.info(f\"Models saved to: {MODEL_DIR}\")\n",
    "    \n",
    "    train_targets = train_df['TARGET'].values\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE MODE: Loading saved models\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    saved_feats = joblib.load(os.path.join(MODEL_DIR, 'feature_list.pkl'))\n",
    "    \n",
    "    missing_feats = [f for f in saved_feats if f not in test_df.columns]\n",
    "    if missing_feats:\n",
    "        print(f\"Warning: {len(missing_feats)} features missing, filling with 0\")\n",
    "        for f in missing_feats:\n",
    "            test_df[f] = 0\n",
    "    \n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    models = []\n",
    "    \n",
    "    with timer(\"Loading models and making predictions\"):\n",
    "        for n_fold in range(num_folds):\n",
    "            model_path = os.path.join(MODEL_DIR, f'lgbm_fold_{n_fold}.txt')\n",
    "            if os.path.exists(model_path):\n",
    "                model = lgb.Booster(model_file=model_path)\n",
    "                models.append(model)\n",
    "                sub_preds += model.predict(test_df[saved_feats]) / num_folds\n",
    "                print(f'Loaded: {model_path}')\n",
    "            else:\n",
    "                print(f'Warning: Not found: {model_path}')\n",
    "    \n",
    "    print(f'\\nLoaded {len(models)} models')\n",
    "    \n",
    "    oof_preds = None\n",
    "    train_targets = None\n",
    "    training_history = None\n",
    "    fold_results = None\n",
    "    feature_importance_df = None\n",
    "\n",
    "test_df['TARGET'] = sub_preds\n",
    "submission_df = test_df[['SK_ID_CURR', 'TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 5: Output & Visualization\n",
    "# =============================================================================\n",
    "\n",
    "# Save submission\n",
    "with timer(\"Saving submission file\"):\n",
    "    submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "    print(f\"Submission saved to: {SUBMISSION_FILE}\")\n",
    "    print(f\"Shape: {submission_df.shape}\")\n",
    "    print(f\"Predictions range: [{submission_df['TARGET'].min():.4f}, {submission_df['TARGET'].max():.4f}]\")\n",
    "\n",
    "# Visualizations only available in TRAIN_MODE\n",
    "if TRAIN_MODE and feature_importance_df is not None:\n",
    "    \n",
    "    # Feature Importance\n",
    "    with timer(\"Generating feature importance\"):\n",
    "        cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "                .groupby(\"feature\").mean()\n",
    "                .sort_values(by=\"importance\", ascending=False)[:40].index)\n",
    "        best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "        plt.figure(figsize=(10, 12))\n",
    "        sns.barplot(x=\"importance\", y=\"feature\", \n",
    "                    data=best_features.sort_values(by=\"importance\", ascending=False), palette=\"viridis\")\n",
    "        plt.title('LightGBM Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('lgbm_importances01.png', dpi=150)\n",
    "        plt.show()\n",
    "    \n",
    "    # Per-fold ROC curves\n",
    "    with timer(\"Generating per-fold ROC curves\"):\n",
    "        n_folds = len(fold_results)\n",
    "        cols = min(5, n_folds)\n",
    "        rows = (n_folds + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "        axes = axes.flatten() if n_folds > 1 else [axes]\n",
    "        colors = plt.cm.viridis(np.linspace(0, 0.9, n_folds))\n",
    "        for idx, fold_data in enumerate(fold_results):\n",
    "            ax = axes[idx]\n",
    "            fpr, tpr, _ = roc_curve(fold_data['y_true'], fold_data['y_pred'])\n",
    "            ax.plot(fpr, tpr, color=colors[idx], lw=2, label=f'AUC = {fold_data[\"auc\"]:.4f}')\n",
    "            ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "            ax.set_title(f'Fold {fold_data[\"fold\"]}')\n",
    "            ax.legend(loc='lower right')\n",
    "            ax.grid(alpha=0.3)\n",
    "        for idx in range(n_folds, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        plt.suptitle('ROC Curves for Each Fold', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curves_per_fold.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Combined ROC\n",
    "    with timer(\"Generating combined ROC curve\"):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for fold_data in fold_results:\n",
    "            fpr, tpr, _ = roc_curve(fold_data['y_true'], fold_data['y_pred'])\n",
    "            plt.plot(fpr, tpr, alpha=0.3, lw=1)\n",
    "        fpr, tpr, _ = roc_curve(train_targets, oof_preds)\n",
    "        oof_auc = roc_auc_score(train_targets, oof_preds)\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Mean OOF (AUC = {oof_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'navy', lw=2, linestyle='--')\n",
    "        plt.title('Combined ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curve.png', dpi=150)\n",
    "        plt.show()\n",
    "    \n",
    "    # Training curves\n",
    "    with timer(\"Generating training curves\"):\n",
    "        n_folds = len(training_history)\n",
    "        cols = min(5, n_folds)\n",
    "        rows = (n_folds + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n",
    "        axes = axes.flatten() if n_folds > 1 else [axes]\n",
    "        for idx, fold_history in enumerate(training_history):\n",
    "            ax = axes[idx]\n",
    "            iterations = range(1, len(fold_history['train_auc']) + 1)\n",
    "            ax.plot(iterations, fold_history['train_auc'], label='Train', color='blue', alpha=0.7)\n",
    "            ax.plot(iterations, fold_history['valid_auc'], label='Valid', color='red', alpha=0.7)\n",
    "            ax.axvline(x=fold_history['best_iteration'], color='green', linestyle='--', alpha=0.7)\n",
    "            ax.set_title(f'Fold {fold_history[\"fold\"]} (best: {fold_history[\"best_iteration\"]})')\n",
    "            ax.legend(loc='lower right', fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "        for idx in range(n_folds, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        plt.suptitle('Training Curves', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for fr in fold_results:\n",
    "        print(f\"  Fold {fr['fold']:2d}: AUC = {fr['auc']:.6f}\")\n",
    "    print(f\"\\n  Mean Fold AUC: {np.mean([fr['auc'] for fr in fold_results]):.6f}\")\n",
    "    print(f\"  OOF AUC:       {oof_auc:.6f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Predictions generated using {len(models)} saved models\")\n",
    "    print(f\"Visualizations skipped (only available in TRAIN_MODE)\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {SUBMISSION_FILE}\")\n",
    "if TRAIN_MODE:\n",
    "    print(f\"  - {MODEL_DIR}lgbm_fold_*.pkl (saved models)\")\n",
    "    print(f\"  - lgbm_importances01.png\")\n",
    "    print(f\"  - roc_curves_per_fold.png\")\n",
    "    print(f\"  - roc_curve.png\")\n",
    "    print(f\"  - training_curves.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
