{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Home Credit Default Risk: Comprehensive EDA Report\n",
        "**Generated by Antigravity Agent**\n",
        "\n",
        "## 0. Goal & Constraints\n",
        "**Goal**: Generate a reproducible, structured, and visual EDA report for the Home Credit Default Risk dataset (`application_train.csv`).\n",
        "**Focus**: Data Quality, Target Distribution, Missingness, Numerical/Categorical distributions, Correlations, and Bias signals.\n",
        "**Constraints**: No missing value imputation, no outlier removal (report only).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_auc_score, mutual_info_score\n",
        "import os\n",
        "import warnings\n",
        "import json\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "# Constants\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_PATH = 'application_train.csv'\n",
        "TEST_PATH = 'application_test.csv'\n",
        "TARGET_COL = 'TARGET'\n",
        "ID_COL = 'SK_ID_CURR'\n",
        "\n",
        "# Output directories\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "os.makedirs('tables', exist_ok=True)\n",
        "\n",
        "print(\"Environment setup complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def save_figure(fig, filename):\n",
        "    fig.savefig(f'figures/{filename}', bbox_inches='tight')\n",
        "    # plt.close(fig) # Keep open for notebook display\n",
        "\n",
        "def save_table(df, filename):\n",
        "    df.to_csv(f'tables/{filename}')\n",
        "    print(f\"Saved table: tables/{filename}\")\n",
        "\n",
        "def plot_distribution(df, col, target_col=TARGET_COL, sample_size=50000):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    \n",
        "    # Sampling for plotting to improve performance\n",
        "    plot_df = df.sample(min(len(df), sample_size), random_state=RANDOM_SEED) if len(df) > sample_size else df\n",
        "    \n",
        "    # Distribution\n",
        "    sns.histplot(data=plot_df, x=col, kde=True, ax=axes[0], color='skyblue')\n",
        "    axes[0].set_title(f'Distribution of {col}')\n",
        "    \n",
        "    # Relation with Target\n",
        "    if df[col].dtype == 'object' or len(df[col].unique()) < 20: \n",
        "        # Categorical or low cardinality\n",
        "        sns.barplot(data=plot_df, x=col, y=target_col, ax=axes[1], errorbar=('ci', 95))\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "    else: \n",
        "        # Numerical\n",
        "        sns.boxplot(data=plot_df, x=target_col, y=col, ax=axes[1])\n",
        "    \n",
        "    axes[1].set_title(f'{col} vs {target_col}')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def calc_psi(expected, actual, buckettype='bins', buckets=10, axis=0):\n",
        "    '''Calculate the PSI (Population Stability Index) for two vectors'''\n",
        "    def scale_range (input, min, max):\n",
        "        input += -(np.min(input))\n",
        "        input /= np.max(input) / (max - min)\n",
        "        input += min\n",
        "        return input\n",
        "\n",
        "    breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
        "\n",
        "    if buckettype == 'bins':\n",
        "        breakpoints = scale_range(breakpoints, np.min(expected), np.max(expected))\n",
        "    elif buckettype == 'quantiles':\n",
        "        breakpoints = np.stack([np.percentile(expected, b) for b in breakpoints])\n",
        "\n",
        "    expected_percents = np.histogram(expected, breakpoints)[0] / len(expected)\n",
        "    actual_percents = np.histogram(actual, breakpoints)[0] / len(actual)\n",
        "\n",
        "    def sub_psi(e_perc, a_perc):\n",
        "        if a_perc == 0: a_perc = 0.0001\n",
        "        if e_perc == 0: e_perc = 0.0001\n",
        "        value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
        "        return(value)\n",
        "\n",
        "    psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n",
        "    return psi_value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter A: Data Loading & Meta Info\n",
        "- **Goal**: Load data, check basic properties (shape, memory, duplicates), and generate a schema summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load Data\n",
        "if os.path.exists(TRAIN_PATH):\n",
        "    df_train = pd.read_csv(TRAIN_PATH)\n",
        "    print(f\"Loaded {TRAIN_PATH}: {df_train.shape}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"{TRAIN_PATH} not found. Please ensure the file is in the working directory.\")\n",
        "\n",
        "# Basic Meta Info\n",
        "memory_usage = df_train.memory_usage(deep=True).sum() / 1024**2\n",
        "print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "# Check ID Uniqueness\n",
        "n_unique_ids = df_train[ID_COL].nunique()\n",
        "n_rows = len(df_train)\n",
        "print(f\"Unique IDs: {n_unique_ids}\")\n",
        "print(f\"Duplicate IDs: {n_rows - n_unique_ids}\")\n",
        "\n",
        "# Schema Table\n",
        "schema_d = []\n",
        "for col in df_train.columns:\n",
        "    n_unique = df_train[col].nunique()\n",
        "    n_missing = df_train[col].isnull().sum()\n",
        "    example_vals = df_train[col].dropna().unique()[:3] if n_unique > 0 else []\n",
        "    \n",
        "    inferred_type = 'numeric'\n",
        "    if df_train[col].dtype == 'object':\n",
        "        inferred_type = 'categorical'\n",
        "    elif 'DAYS' in col or 'DATE' in col:  # Rough heuristic\n",
        "        inferred_type = 'date-like'\n",
        "        \n",
        "    schema_d.append({\n",
        "        'col_name': col,\n",
        "        'dtype': df_train[col].dtype,\n",
        "        'n_unique': n_unique,\n",
        "        'pct_unique': n_unique / n_rows,\n",
        "        'n_missing': n_missing,\n",
        "        'pct_missing': n_missing / n_rows,\n",
        "        'example_values': str(list(example_vals)),\n",
        "        'inferred_type': inferred_type\n",
        "    })\n",
        "\n",
        "df_schema = pd.DataFrame(schema_d)\n",
        "save_table(df_schema, 'schema_table.csv')\n",
        "display(df_schema.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter B: Target Variable Overview\n",
        "- **Goal**: Analyze the distribution and imbalance of the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Target Distribution\n",
        "target_counts = df_train[TARGET_COL].value_counts()\n",
        "target_pct = df_train[TARGET_COL].value_counts(normalize=True)\n",
        "\n",
        "print(\"Target Distribution:\")\n",
        "print(target_counts)\n",
        "print(\"\\nTarget Percentage:\")\n",
        "print(target_pct)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.countplot(x=TARGET_COL, data=df_train)\n",
        "plt.title('Target Variable Distribution (Imbalance)')\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}\\n({p.get_height()/n_rows:.1%})', \n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
        "                ha='center', va='bottom')\n",
        "save_figure(plt.gcf(), 'chapter_b_target_dist.png')\n",
        "plt.show()\n",
        "\n",
        "# Baseline Metrics\n",
        "majority_class_acc = target_pct.max()\n",
        "prevalence = target_pct[1] if 1 in target_pct else 0\n",
        "print(f\"Majority Class Baseline Accuracy: {majority_class_acc:.4f}\")\n",
        "print(f\"Positive Class Prevalence: {prevalence:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter C: Missingness Intelligence\n",
        "- **Goal**: Analyze missing value patterns and their relationship with the target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Top Missing Columns\n",
        "missing_ranking = df_schema.sort_values('pct_missing', ascending=False)\n",
        "top_missing = missing_ranking[missing_ranking['pct_missing'] > 0].head(30)\n",
        "display(top_missing[['col_name', 'pct_missing', 'n_missing']])\n",
        "\n",
        "# Plot Top Missing\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(data=top_missing, y='col_name', x='pct_missing')\n",
        "plt.title('Top 30 Missing Features')\n",
        "save_figure(plt.gcf(), 'chapter_c_missing_top30.png')\n",
        "plt.show()\n",
        "\n",
        "# Row-wise Missing Count\n",
        "df_train['n_missing_row'] = df_train.isnull().sum(axis=1)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df_train['n_missing_row'], bins=30)\n",
        "plt.title('Distribution of Missing Values per Row')\n",
        "save_figure(plt.gcf(), 'chapter_c_row_missingness.png')\n",
        "plt.show()\n",
        "\n",
        "# Missingness vs Target (Lift Analysis)\n",
        "# Identify columns with meaningful missingness (> 5% missing)\n",
        "high_missing_cols = missing_ranking[missing_ranking['pct_missing'] > 0.05]['col_name'].tolist()\n",
        "\n",
        "missing_signals = []\n",
        "for col in high_missing_cols[:20]: # Limit to top 20 to save time\n",
        "    is_missing = df_train[col].isnull()\n",
        "    target_rate_missing = df_train[is_missing][TARGET_COL].mean()\n",
        "    target_rate_present = df_train[~is_missing][TARGET_COL].mean()\n",
        "    \n",
        "    missing_signals.append({\n",
        "        'feature': col,\n",
        "        'missing_rate': is_missing.mean(),\n",
        "        'target_rate_missing': target_rate_missing,\n",
        "        'target_rate_present': target_rate_present,\n",
        "        'lift': target_rate_missing / (target_rate_present + 1e-6)\n",
        "    })\n",
        "\n",
        "df_missing_signals = pd.DataFrame(missing_signals).sort_values('lift', ascending=False)\n",
        "save_table(df_missing_signals, 'missingness_signals.csv')\n",
        "display(df_missing_signals.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter D: Numerical Features EDA\n",
        "- **Goal**: Distribution, Statistics, and Target Relationship for numerical features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Identify Numerical Columns\n",
        "numeric_cols = df_schema[df_schema['inferred_type'] == 'numeric']['col_name'].tolist()\n",
        "# Exclude ID and Target\n",
        "numeric_cols = [c for c in numeric_cols if c not in [ID_COL, TARGET_COL, 'n_missing_row']]\n",
        "\n",
        "# D1. Numerical Summary\n",
        "num_summary = df_train[numeric_cols].describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]).T\n",
        "num_summary['missing%'] = df_train[numeric_cols].isnull().mean()\n",
        "num_summary['skew'] = df_train[numeric_cols].skew()\n",
        "num_summary['kurtosis'] = df_train[numeric_cols].kurtosis()\n",
        "save_table(num_summary, 'numeric_summary.csv')\n",
        "display(num_summary.head())\n",
        "\n",
        "# D3. Numeric Signal Ranking (AUC/KS)\n",
        "numeric_signals = []\n",
        "for col in numeric_cols:\n",
        "    # Drop NA for calculation\n",
        "    valid_data = df_train[[col, TARGET_COL]].dropna()\n",
        "    if len(valid_data) < 100 or valid_data[col].nunique() < 2:\n",
        "        continue\n",
        "        \n",
        "    try:\n",
        "        # Simple AUC (using feature as score)\n",
        "        auc = roc_auc_score(valid_data[TARGET_COL], valid_data[col])\n",
        "        # If AUC < 0.5, flip it (feature might be negatively correlated)\n",
        "        auc = max(auc, 1-auc)\n",
        "        \n",
        "        # KS Statistic\n",
        "        s0 = valid_data[valid_data[TARGET_COL]==0][col]\n",
        "        s1 = valid_data[valid_data[TARGET_COL]==1][col]\n",
        "        ks_stat, _ = stats.ks_2samp(s0, s1)\n",
        "        \n",
        "        numeric_signals.append({\n",
        "            'feature': col,\n",
        "            'auc': auc,\n",
        "            'ks_stat': ks_stat\n",
        "        })\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "df_num_signals = pd.DataFrame(numeric_signals).sort_values('auc', ascending=False)\n",
        "save_table(df_num_signals, 'numeric_signal_ranking.csv')\n",
        "print(\"Top Numeric Signals by AUC:\")\n",
        "display(df_num_signals.head(10))\n",
        "\n",
        "# Plot Top 5 Features\n",
        "for col in df_num_signals['feature'].head(5):\n",
        "    fig = plot_distribution(df_train, col)\n",
        "    save_figure(fig, f'chapter_d_dist_{col}.png')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter E: Categorical Features EDA\n",
        "- **Goal**: Analyze categorical features, cardinality, and target rates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "cat_cols = df_schema[df_schema['inferred_type'] == 'categorical']['col_name'].tolist()\n",
        "\n",
        "# E1 & E2. Categorical Stats & Ranking\n",
        "cat_stats = []\n",
        "\n",
        "for col in cat_cols:\n",
        "    total_count = len(df_train)\n",
        "    n_unique = df_train[col].nunique()\n",
        "    \n",
        "    # Calculate Target Rate Variance (weighted) or Mutual Information (simple proxy)\n",
        "    # Here we look at the range of target rates across categories\n",
        "    \n",
        "    # Group by category\n",
        "    grp = df_train.groupby(col)[TARGET_COL].agg(['count', 'mean'])\n",
        "    grp = grp.sort_values('count', ascending=False)\n",
        "    \n",
        "    # Check for rare classes (<1%)\n",
        "    rare_mask = (grp['count'] / total_count) < 0.01\n",
        "    n_rare = rare_mask.sum()\n",
        "    pct_rare = grp[rare_mask]['count'].sum() / total_count\n",
        "    \n",
        "    # Signal strength: Range of target rates (max - min) for categories with meaningful size (>100 samples)\n",
        "    valid_cats = grp[grp['count'] > 100]\n",
        "    tr_range = 0\n",
        "    if len(valid_cats) > 1:\n",
        "        tr_range = valid_cats['mean'].max() - valid_cats['mean'].min()\n",
        "        \n",
        "    cat_stats.append({\n",
        "        'feature': col,\n",
        "        'n_unique': n_unique,\n",
        "        'n_rare_cats': n_rare,\n",
        "        'pct_rare_samples': pct_rare,\n",
        "        'target_rate_range': tr_range\n",
        "    })\n",
        "    \n",
        "    # Plot Top categories\n",
        "    if n_unique <= 50: # Limit plots\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        # Top 10 categories\n",
        "        top_cats = grp.head(10).index\n",
        "        plot_data = df_train[df_train[col].isin(top_cats)]\n",
        "        \n",
        "        sns.barplot(data=plot_data, x=col, y=TARGET_COL, order=top_cats, errorbar=('ci', 95))\n",
        "        plt.title(f'{col} Default Rate (Top Categories)')\n",
        "        plt.xticks(rotation=45)\n",
        "        save_figure(plt.gcf(), f'chapter_e_{col}_target_rate.png')\n",
        "        plt.close()\n",
        "\n",
        "df_cat_stats = pd.DataFrame(cat_stats).sort_values('target_rate_range', ascending=False)\n",
        "save_table(df_cat_stats, 'categorical_signal_ranking.csv')\n",
        "display(df_cat_stats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter F: Correlation & Redundancy\n",
        "- **Goal**: Identify collinearity and strong correlations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Numerical Correlation (Top Features)\n",
        "# Use top 30 numeric features by signal to avoid huge matrix\n",
        "top_numeric = df_num_signals.head(30)['feature'].tolist()\n",
        "corr_mat = df_train[top_numeric].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_mat, cmap='coolwarm', center=0, annot=False)\n",
        "plt.title('Correlation Matrix (Top 30 Numeric Features)')\n",
        "save_figure(plt.gcf(), 'chapter_f_corr_matrix.png')\n",
        "plt.show()\n",
        "\n",
        "# List strong pairs\n",
        "corr_pairs = (corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(bool))\n",
        "              .stack()\n",
        "              .to_frame('corr')\n",
        "              .reset_index())\n",
        "corr_pairs.columns = ['feat_a', 'feat_b', 'corr']\n",
        "corr_pairs['abs_corr'] = corr_pairs['corr'].abs()\n",
        "high_corr = corr_pairs[corr_pairs['abs_corr'] > 0.7].sort_values('abs_corr', ascending=False)\n",
        "save_table(high_corr, 'high_correlation_pairs.csv')\n",
        "display(high_corr.head(15))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter G: Outliers & Sentinel Values\n",
        "- **Goal**: Scan for potential sentinel values (e.g. 365243 in DAYS columns) and extreme outliers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sentinel_candidates = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    # Check for specific value heavily repeated (Mode) that is at the edge\n",
        "    mode_val = df_train[col].mode()[0]\n",
        "    mode_count = (df_train[col] == mode_val).sum()\n",
        "    mode_pct = mode_count / len(df_train)\n",
        "    \n",
        "    # Check if max or min is far from percentiles\n",
        "    desc = num_summary.loc[col]\n",
        "    \n",
        "    is_suspicious = False\n",
        "    note = \"\"\n",
        "    \n",
        "    # Case: Days Employed 365243\n",
        "    if col == 'DAYS_EMPLOYED' and 365243 in df_train[col].values:\n",
        "        is_suspicious = True\n",
        "        note = \"Known Sentinel: 365243\"\n",
        "    \n",
        "    # Case: Mode distinct from median/mean and high pct\n",
        "    if mode_pct > 0.05 and abs(mode_val - desc['50%']) > desc['std']:\n",
        "        is_suspicious = True\n",
        "        note += f\" | High freq mode: {mode_val} ({mode_pct:.1%})\"\n",
        "        \n",
        "    if is_suspicious:\n",
        "        sentinel_candidates.append({\n",
        "            'feature': col,\n",
        "            'suspicious_value': mode_val, \n",
        "            'count': mode_count,\n",
        "            'pct': mode_pct,\n",
        "            'notes': note\n",
        "        })\n",
        "\n",
        "df_sentinels = pd.DataFrame(sentinel_candidates)\n",
        "save_table(df_sentinels, 'potential_sentinels.csv')\n",
        "display(df_sentinels)\n",
        "\n",
        "# Visualize Sentinel vs Target for DAYS_EMPLOYED if exists\n",
        "if 'DAYS_EMPLOYED' in df_train.columns:\n",
        "    df_train['is_365243'] = df_train['DAYS_EMPLOYED'] == 365243\n",
        "    print(df_train.groupby('is_365243')[TARGET_COL].agg(['count', 'mean']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter H: Train vs Test Drift\n",
        "- **Goal**: Compare distribution between Train and Test sets (if Test set exists).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if os.path.exists(TEST_PATH):\n",
        "    df_test = pd.read_csv(TEST_PATH)\n",
        "    print(f\"Loaded Test Data: {df_test.shape}\")\n",
        "    \n",
        "    drift_results = []\n",
        "    \n",
        "    # Check Top Numeric Features for Drift using KS\n",
        "    for col in top_numeric[:20]: # Check top signals\n",
        "        if col in df_test.columns:\n",
        "            s_train = df_train[col].dropna()\n",
        "            s_test = df_test[col].dropna()\n",
        "            \n",
        "            ks_stat, p_val = stats.ks_2samp(s_train, s_test)\n",
        "            drift_results.append({\n",
        "                'feature': col,\n",
        "                'metric': 'KS',\n",
        "                'value': ks_stat,\n",
        "                'drift_detected': ks_stat > 0.1 # Threshold\n",
        "            })\n",
        "            \n",
        "    df_drift = pd.DataFrame(drift_results).sort_values('value', ascending=False)\n",
        "    save_table(df_drift, 'drift_analysis.csv')\n",
        "    display(df_drift.head())\n",
        "else:\n",
        "    print(\"Application Test file not found. Skipping Drift Analysis.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Chapter I: Bias / Fairness EDA\n",
        "- **Goal**: Explore default rates across sensitive attributes (Gender, Age).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 1. Gender\n",
        "if 'CODE_GENDER' in df_train.columns:\n",
        "    print(\"Gender Analysis:\")\n",
        "    g_grp = df_train.groupby('CODE_GENDER')[TARGET_COL].agg(['count', 'mean'])\n",
        "    display(g_grp)\n",
        "    \n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.barplot(data=df_train, x='CODE_GENDER', y=TARGET_COL, errorbar=('ci', 95))\n",
        "    plt.title('Default Rate by Gender')\n",
        "    save_figure(plt.gcf(), 'chapter_i_gender_bias.png')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Age (Binning DAYS_BIRTH)\n",
        "if 'DAYS_BIRTH' in df_train.columns:\n",
        "    df_train['AGE'] = df_train['DAYS_BIRTH'] / -365\n",
        "    df_train['AGE_BIN'] = pd.cut(df_train['AGE'], bins=[20, 30, 40, 50, 60, 70, 100])\n",
        "    \n",
        "    print(\"Age Analysis:\")\n",
        "    age_grp = df_train.groupby('AGE_BIN')[TARGET_COL].agg(['count', 'mean'])\n",
        "    display(age_grp)\n",
        "    \n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.barplot(data=df_train, x='AGE_BIN', y=TARGET_COL, errorbar=('ci', 95))\n",
        "    plt.title('Default Rate by Age Group')\n",
        "    save_figure(plt.gcf(), 'chapter_i_age_bias.png')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Executive Summary & Outputs\n",
        "The analysis is complete. Key tables have been saved to the `tables/` directory and figures to `figures/`.\n",
        "summary.json will be generated now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "summary_json = {\n",
        "    \"n_rows\": int(len(df_train)),\n",
        "    \"n_cols\": int(len(df_train.columns)),\n",
        "    \"target_prevalence\": float(prevalence),\n",
        "    \"top_missing_cols\": top_missing['col_name'].tolist()[:5],\n",
        "    \"top_numeric_signals\": df_num_signals['feature'].tolist()[:5] if 'df_num_signals' in locals() else [],\n",
        "    \"top_categorical_signals\": df_cat_stats['feature'].tolist()[:5] if 'df_cat_stats' in locals() else []\n",
        "}\n",
        "\n",
        "with open('summary.json', 'w') as f:\n",
        "    json.dump(summary_json, f, indent=4)\n",
        "\n",
        "print(\"Summary JSON saved.\")\n",
        "print(\"To generate HTML report, try using: jupyter nbconvert --to html eda_report.ipynb\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}