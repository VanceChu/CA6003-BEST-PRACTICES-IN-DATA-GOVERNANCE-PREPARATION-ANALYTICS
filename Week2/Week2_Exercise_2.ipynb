{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Feature Engineering - Data Encoding, Scaling, and Bias Mitigation\n",
    "\n",
    "**Objective**: Explore the House Prices dataset, perform systematic feature engineering (encoding, transformation, scaling), and understand their impact on model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the House Prices dataset\n",
    "# Note: Adjust the path if necessary based on your folder structure\n",
    "file_path = \"house-prices-advanced-regression-techniques/train.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}. Please check the path.\")\n",
    "\n",
    "# 2. Inspect the dataset structure\n",
    "print(\"Shape:\", df.shape)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Type Separation (Numerical and Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separate features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 2. Create lists (already done above, but explicit printing)\n",
    "print(f\"Numerical Features ({len(numerical_features)}):\", numerical_features)\n",
    "print(f\"Categorical Features ({len(categorical_features)}):\", categorical_features)\n",
    "\n",
    "# Justification:\n",
    "# Separating features is crucial because they require different preprocessing steps.\n",
    "# Numerical features typically need scaling and skewness correction.\n",
    "# Categorical features need encoding (e.g., One-Hot, Ordinal) to be used by machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Numerical Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute correlation matrix\n",
    "corr_matrix = df[numerical_features].corr()\n",
    "\n",
    "# 2. Visualize with heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', square=True)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Focus on relationship with 'SalePrice'\n",
    "print(\"Top 10 features correlated with SalePrice:\\n\", corr_matrix['SalePrice'].sort_values(ascending=False).head(10))\n",
    "\n",
    "# Key Observations:\n",
    "# 1. OverallQual has the strongest positive correlation with SalePrice.\n",
    "# 2. GrLivArea (Living Area) is also highly correlated, indicating size matters.\n",
    "# 3. GarageCars/GarageArea are correlated with each other (multicollinearity) and with Price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Identify Top 10 Numerical Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute absolute correlation\n",
    "abs_corr = df[numerical_features].corr()['SalePrice'].abs()\n",
    "\n",
    "# 2. Exclude 'SalePrice'\n",
    "abs_corr = abs_corr.drop('SalePrice')\n",
    "\n",
    "# 3. Identify top 10\n",
    "top_10_num_drivers = abs_corr.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Deliverable: Table\n",
    "pd.DataFrame({'Feature': top_10_num_drivers.index, 'Abs_Correlation': top_10_num_drivers.values})\n",
    "\n",
    "# Metric Justification: Absolute correlation is used because both strong positive and strong negative relationships are important predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: The \"Hidden\" Categorical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Examine Top 10 numerical features\n",
    "print(\"Top 10 Numerical Drivers:\", top_10_num_drivers.index.tolist())\n",
    "\n",
    "# 2. Identify hidden categorical feature: 'OverallQual' (Overall Quality) is often encoded as 1-10 but represents ordinal categories.\n",
    "hidden_cat_feature = 'OverallQual'\n",
    "\n",
    "# 3. Analysis\n",
    "# a. Type: Ordinal (1=Very Poor, 10=Very Excellent)\n",
    "# b. Plot against SalePrice\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=hidden_cat_feature, y='SalePrice', data=df)\n",
    "plt.title(f\"{hidden_cat_feature} vs SalePrice\")\n",
    "plt.show()\n",
    "\n",
    "# c. Comment on implicit encoding: The dataset uses integer encoding (1-10) which preserves the natural order. This is suitable for tree-based models and often linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inspect Pure Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Examine remaining categorical columns\n",
    "cat_cardinality = df[categorical_features].nunique().sort_values(ascending=False)\n",
    "\n",
    "# 2. Check Cardinality\n",
    "print(\"Cardinality of Categorical Features:\")\n",
    "print(cat_cardinality.head(10))\n",
    "\n",
    "# Thinking about encoding:\n",
    "# High Cardinality (e.g., Neighborhood) -> Target Encoding or Frequency Encoding might be better than One-Hot.\n",
    "# Low Cardinality -> One-Hot Encoding is standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Prepare data for RF (needs numbers)\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Simple preprocessing for Feature Importance check\n",
    "# Identify cols again\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_num = pd.DataFrame(num_imputer.fit_transform(X[num_cols]), columns=num_cols)\n",
    "X_cat = pd.DataFrame(cat_imputer.fit_transform(X[cat_cols]), columns=cat_cols)\n",
    "\n",
    "# Ordinal Encode Categorical for RF\n",
    "encoder = OrdinalEncoder()\n",
    "X_cat_encoded = pd.DataFrame(encoder.fit_transform(X_cat), columns=cat_cols)\n",
    "\n",
    "X_rf = pd.concat([X_num, X_cat_encoded], axis=1)\n",
    "\n",
    "# 1. Train Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_rf, y)\n",
    "\n",
    "# 2. Get top 10 features\n",
    "importances = pd.Series(rf.feature_importances_, index=X_rf.columns).sort_values(ascending=False)\n",
    "top_10_rf_features = importances.head(10)\n",
    "print(\"Top 10 Random Forest Features:\\n\", top_10_rf_features)\n",
    "\n",
    "# Plot Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10_rf_features.plot(kind='barh')\n",
    "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Random Forest helps uncover non-linear effects because it uses decision trees that can capture complex interactions and non-monotonic relationships that linear correlation misses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top categorical features against SalePrice\n",
    "top_cat_features = [f for f in top_10_rf_features.index if f in categorical_features]\n",
    "print(f\"\\nTop Categorical Features from RF: {top_cat_features}\")\n",
    "\n",
    "# Plot up to 5 categorical features\n",
    "for feat in top_cat_features[:min(5, len(top_cat_features))]:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=feat, y='SalePrice', data=df)\n",
    "    plt.title(f\"{feat} vs SalePrice (Random Forest Important Feature)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Encoding Strategy and Rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select top 5 categorical features (based on RF importance or domain knowledge from analysis)\n",
    "top_cat_from_rf = [f for f in importances.index if f in categorical_features][:5]\n",
    "if len(top_cat_from_rf) < 5:\n",
    "    rest = [c for c in cat_cardinality.index if c not in top_cat_from_rf][:5-len(top_cat_from_rf)]\n",
    "    top_cat_from_rf.extend(rest)\n",
    "    \n",
    "print(\"Selected Categorical Features for Encoding:\", top_cat_from_rf)\n",
    "\n",
    "# 2. Encoding Strategy for each feature\n",
    "encoding_results = {}\n",
    "\n",
    "for feat in top_cat_from_rf:\n",
    "    print(f\"\\n--- Encoding {feat} ---\")\n",
    "    \n",
    "    # Check cardinality\n",
    "    n_unique = df[feat].nunique()\n",
    "    print(f\"Cardinality: {n_unique}\")\n",
    "    \n",
    "    # Decision logic\n",
    "    if n_unique > 10:\n",
    "        print(f\"Strategy: High cardinality → Frequency Encoding\")\n",
    "        # Frequency encoding\n",
    "        freq_map = df[feat].value_counts(normalize=True).to_dict()\n",
    "        encoding_results[feat] = df[feat].map(freq_map)\n",
    "    else:\n",
    "        print(f\"Strategy: Low cardinality → One-Hot Encoding\")\n",
    "        # One-hot encoding\n",
    "        encoding_results[feat] = pd.get_dummies(df[feat], prefix=feat)\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"Sample output:\")\n",
    "    if isinstance(encoding_results[feat], pd.DataFrame):\n",
    "        display(encoding_results[feat].head())\n",
    "    else:\n",
    "        display(encoding_results[feat].head())\n",
    "\n",
    "print(\"\\nEncoding complete for all 5 features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Data Transformation and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all high-skewness numerical features\n",
    "print(\"=== Identifying High-Skewness Features ===\")\n",
    "\n",
    "# Calculate skewness for all numerical features (excluding target)\n",
    "numerical_features_for_skew = [f for f in numerical_features if f != 'SalePrice']\n",
    "skewness_data = df[numerical_features_for_skew].skew().sort_values(ascending=False)\n",
    "\n",
    "# Define threshold (commonly |skewness| > 0.5 or 1.0)\n",
    "skew_threshold = 0.75\n",
    "high_skew_features = skewness_data[abs(skewness_data) > skew_threshold]\n",
    "\n",
    "print(f\"\\nTotal numerical features: {len(numerical_features_for_skew)}\")\n",
    "print(f\"Features with |skewness| > {skew_threshold}: {len(high_skew_features)}\")\n",
    "print(f\"\\nTop 10 most skewed features:\")\n",
    "print(high_skew_features.head(10))\n",
    "\n",
    "# Create a summary table\n",
    "skew_summary = pd.DataFrame({\n",
    "    'Feature': high_skew_features.index,\n",
    "    'Skewness': high_skew_features.values,\n",
    "    'Abs_Skewness': abs(high_skew_features.values)\n",
    "}).sort_values('Abs_Skewness', ascending=False)\n",
    "\n",
    "print(f\"\\n=== All High-Skewness Features (|skew| > {skew_threshold}) ===\")\n",
    "display(skew_summary.head(15))\n",
    "\n",
    "# Note the key features mentioned in PDF\n",
    "pdf_features = ['SalePrice', 'GrLivArea', 'TotalBsmtSF']\n",
    "print(f\"\\n=== Skewness of PDF-mentioned features ===\")\n",
    "for feat in pdf_features:\n",
    "    if feat in df.columns:\n",
    "        print(f\"{feat}: {df[feat].skew():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# 1. Find Skew\n",
    "target_skew = df['SalePrice'].skew()\n",
    "print(f\"SalePrice Skew: {target_skew}\")\n",
    "\n",
    "# 2. Transformations\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "sp_log = np.log1p(df['SalePrice'])\n",
    "sp_pt = pt.fit_transform(df[['SalePrice']])\n",
    "\n",
    "# 3. Compare\n",
    "print(f\"Log Transform Skew: {sp_log.skew()}\")\n",
    "print(f\"Yeo-Johnson Skew: {pd.Series(sp_pt.flatten()).skew()}\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1); sns.histplot(df['SalePrice'], kde=True); plt.title('Original')\n",
    "plt.subplot(1, 3, 2); sns.histplot(sp_log, kde=True); plt.title('Log Transformed')\n",
    "plt.subplot(1, 3, 3); sns.histplot(sp_pt, kde=True); plt.title('Yeo-Johnson')\n",
    "plt.show()\n",
    "\n",
    "# 4. Scaling Methods Comparison (using GrLivArea as example)\n",
    "feature_to_scale = df[['GrLivArea']].dropna()\n",
    "\n",
    "scalers = {\n",
    "    'Standard': StandardScaler(),\n",
    "    'MinMax': MinMaxScaler(),\n",
    "    'Robust': RobustScaler()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, (name, scaler) in enumerate(scalers.items()):\n",
    "    scaled_data = scaler.fit_transform(feature_to_scale)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(scaled_data, kde=True)\n",
    "    plt.title(f\"{name} Scaler\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendation: RobustScaler is often preferred if there are outliers (as seen in GrLivArea often). StandardScaler is standard for linear models if data is Gaussian-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional: Box-Cox Transformation (requires strictly positive values)\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Box-Cox for SalePrice (which is always positive)\n",
    "pt_boxcox = PowerTransformer(method='box-cox')\n",
    "sp_boxcox = pt_boxcox.fit_transform(df[['SalePrice']])\n",
    "\n",
    "print(f\"Box-Cox Skew: {pd.Series(sp_boxcox.flatten()).skew()}\")\n",
    "\n",
    "# Visualize all three transformations\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 4, 1); sns.histplot(df['SalePrice'], kde=True); plt.title('Original')\n",
    "plt.subplot(1, 4, 2); sns.histplot(sp_log, kde=True); plt.title('Log Transform')\n",
    "plt.subplot(1, 4, 3); sns.histplot(sp_boxcox, kde=True); plt.title('Box-Cox Transform')\n",
    "plt.subplot(1, 4, 4); sns.histplot(sp_pt, kde=True); plt.title('Yeo-Johnson Transform')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TotalBsmtSF (as requested in PDF)\n",
    "if 'TotalBsmtSF' in df.columns:\n",
    "    print(\"\\n=== TotalBsmtSF Analysis ===\")\n",
    "    totalbsmt_skew = df['TotalBsmtSF'].skew()\n",
    "    print(f\"TotalBsmtSF Original Skew: {totalbsmt_skew}\")\n",
    "    \n",
    "    # Apply transformations\n",
    "    # Handle zeros by adding 1 before log\n",
    "    tbsf_log = np.log1p(df['TotalBsmtSF'])\n",
    "    print(f\"After Log Transform: {tbsf_log.skew()}\")\n",
    "    \n",
    "    # Yeo-Johnson (handles zeros)\n",
    "    pt_yj = PowerTransformer(method='yeo-johnson')\n",
    "    tbsf_yj = pt_yj.fit_transform(df[['TotalBsmtSF']])\n",
    "    print(f\"After Yeo-Johnson: {pd.Series(tbsf_yj.flatten()).skew()}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.subplot(1, 3, 1); sns.histplot(df['TotalBsmtSF'], kde=True); plt.title(f'TotalBsmtSF Original\\nSkew: {totalbsmt_skew:.2f}')\n",
    "    plt.subplot(1, 3, 2); sns.histplot(tbsf_log, kde=True); plt.title(f'Log Transform\\nSkew: {tbsf_log.skew():.2f}')\n",
    "    plt.subplot(1, 3, 3); sns.histplot(tbsf_yj, kde=True); plt.title(f'Yeo-Johnson\\nSkew: {pd.Series(tbsf_yj.flatten()).skew():.2f}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"TotalBsmtSF not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MaxAbsScaler to comparison\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "feature_to_scale = df[['GrLivArea']].dropna()\n",
    "\n",
    "scalers_complete = {\n",
    "    'Standard': StandardScaler(),\n",
    "    'MinMax': MinMaxScaler(),\n",
    "    'Robust': RobustScaler(),\n",
    "    'MaxAbs': MaxAbsScaler()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, (name, scaler) in enumerate(scalers_complete.items()):\n",
    "    scaled_data = scaler.fit_transform(feature_to_scale)\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    sns.histplot(scaled_data, kde=True)\n",
    "    plt.title(f\"{name} Scaler\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScaler Recommendations:\")\n",
    "print(\"- StandardScaler: Best for normally distributed data without outliers\")\n",
    "print(\"- MinMaxScaler: When you need a specific range [0,1], but sensitive to outliers\")\n",
    "print(\"- RobustScaler: When data has outliers (uses median and IQR)\")\n",
    "print(\"- MaxAbsScaler: When you need range [-1,1] and want to preserve sparsity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive skewness comparison table\n",
    "skew_comparison = pd.DataFrame({\n",
    "    'Feature': ['SalePrice', 'SalePrice', 'SalePrice', 'TotalBsmtSF', 'TotalBsmtSF'],\n",
    "    'Transformation': ['Original', 'Log', 'Yeo-Johnson', 'Original', 'Log'],\n",
    "    'Skewness': [\n",
    "        df['SalePrice'].skew(),\n",
    "        sp_log.skew(),\n",
    "        pd.Series(sp_pt.flatten()).skew(),\n",
    "        df['TotalBsmtSF'].skew() if 'TotalBsmtSF' in df.columns else None,\n",
    "        tbsf_log.skew() if 'TotalBsmtSF' in df.columns else None\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Skewness Comparison Table ===\")\n",
    "display(skew_comparison)\n",
    "print(\"\\nConclusion: Power transformations (Log, Box-Cox, Yeo-Johnson) effectively reduce skewness close to 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data Prep - Drop rows with missing target\n",
    "df_clean = df.dropna(subset=['SalePrice'])\n",
    "y = df_clean['SalePrice']\n",
    "X = df_clean.drop('SalePrice', axis=1)\n",
    "\n",
    "# Select only numerical cols for baseline to simplify\n",
    "X_baseline = X.select_dtypes(include=[np.number]).fillna(0) # Simple fill\n",
    "\n",
    "# Split\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_baseline, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Baseline Model\n",
    "lr_base = LinearRegression()\n",
    "lr_base.fit(X_train_b, y_train_b)\n",
    "y_pred_b = lr_base.predict(X_test_b)\n",
    "rmse_b = np.sqrt(mean_squared_error(y_test_b, y_pred_b))\n",
    "r2_b = r2_score(y_test_b, y_pred_b)\n",
    "\n",
    "# 2. Improved Model (Log Target + Scaled Features)\n",
    "# Log Transform Target\n",
    "y_train_imp = np.log1p(y_train_b)\n",
    "y_test_imp = np.log1p(y_test_b)\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train_imp = scaler.fit_transform(X_train_b)\n",
    "X_test_imp = scaler.transform(X_test_b)\n",
    "\n",
    "lr_imp = LinearRegression()\n",
    "lr_imp.fit(X_train_imp, y_train_imp)\n",
    "y_pred_imp_log = lr_imp.predict(X_test_imp)\n",
    "y_pred_imp = np.expm1(y_pred_imp_log) # Inverse log\n",
    "\n",
    "rmse_imp = np.sqrt(mean_squared_error(y_test_b, y_pred_imp))\n",
    "r2_imp = r2_score(y_test_b, y_pred_imp)\n",
    "\n",
    "# 4. Compare\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Improved (Log+Scaled)'],\n",
    "    'RMSE': [rmse_b, rmse_imp],\n",
    "    'R2': [r2_b, r2_imp]\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Commentary:\n",
    "# Linear models typically perform better when the target variable is normally distributed (skewness corrected) and features are on the same scale.\n",
    "# Log transformation of SalePrice is highly effective because prices are usually log-normally distributed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
