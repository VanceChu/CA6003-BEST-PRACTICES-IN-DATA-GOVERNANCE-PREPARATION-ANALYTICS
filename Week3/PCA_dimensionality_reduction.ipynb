{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Dimesionality Reduction using Pricipal component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#loading digits data set which are images 64x64\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data          # shape: (n_samples, 64)\n",
    "y = digits.target        # labels: 0..9\n",
    "images = digits.images   # shape: (n_samples, 8, 8)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Each image has 8x8 = 64 pixels/features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Plot some random digits with label to see how good they are . These are given for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 6, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(images[i], cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {y[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### These are tiny 8×8 pictures. The computer sees them as 64 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Why scaling matters for PCA\n",
    "\n",
    "#### PCA is based on variance. Scaling ensures features are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### PCA works best when features are standardized (mean 0, std 1), especially when feature scales differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "##### What PCA needs is that it assumes data is centered around zero (mean =zero) and Variance is measured around the mean\n",
    "##### This can be made by standard scaler only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using only two components\n",
    "pca_2 = PCA(n_components=2, random_state=42)\n",
    "X_2d = pca_2.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Explained variance by PC1 and PC2:\", pca_2.explained_variance_ratio_)\n",
    "print(\"Total explained variance (2D):\", pca_2.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, s=15, alpha=0.8)\n",
    "plt.xlabel(\"PC1 (most important summary direction)\")\n",
    "plt.ylabel(\"PC2 (second most important summary direction)\")\n",
    "plt.title(\"Digits projected from 64D → 2D using PCA\")\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label(\"Digit label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA().fit(X_scaled)\n",
    "cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cum_var, marker=\"o\", markersize=3)\n",
    "plt.xlabel(\"Number of PCA components kept\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"How much information is kept as we add more components?\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Common checkpoints:\n",
    "for k in [2, 10, 20, 30, 40]:\n",
    "    print(f\"{k} components keep about {cum_var[k-1]*100:.1f}% of variance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reconstruct(X_scaled, n_components):\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_reduced = pca.fit_transform(X_scaled)\n",
    "    X_recon = pca.inverse_transform(X_reduced)\n",
    "    return X_reduced, X_recon, pca\n",
    "\n",
    "components_list = [2, 10, 20, 40]\n",
    "recons = {}\n",
    "\n",
    "for k in components_list:\n",
    "    _, X_recon, pca_k = pca_reconstruct(X_scaled, k)\n",
    "    recons[k] = (X_recon, pca_k)\n",
    "    print(f\"{k} components keep {pca_k.explained_variance_ratio_.sum()*100:.1f}% variance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = 8  # number of digits to display\n",
    "\n",
    "fig, axes = plt.subplots(len(components_list) + 1, n_show, figsize=(12, 6))\n",
    "\n",
    "# Row 1: original\n",
    "for i in range(n_show):\n",
    "    axes[0, i].imshow(X_scaled[i].reshape(8, 8), cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"Orig: {y[i]}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "axes[0, 0].set_ylabel(\"Original\", rotation=90, labelpad=30)\n",
    "\n",
    "# Rows: reconstructions\n",
    "for row, k in enumerate(components_list, start=1):\n",
    "    X_recon, _ = recons[k]\n",
    "    for i in range(n_show):\n",
    "        axes[row, i].imshow(X_recon[i].reshape(8, 8), cmap=\"gray\")\n",
    "        axes[row, i].axis(\"off\")\n",
    "    axes[row, 0].set_ylabel(f\"{k} PCs\", rotation=90, labelpad=30)\n",
    "\n",
    "plt.suptitle(\"What gets lost when we reduce dimensions? (Reconstruction)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "##### The first row shows the original images, while rows 2–5 show different images reconstructed after compressing each one into just 2 PCA components, revealing how much visual information is lost when dimensionality is reduced too aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### With 2 components, digits look blurry and lose detail\n",
    "\n",
    "#### With 10–20 components, digits become recognizable\n",
    "\n",
    "#### With 40 components, digits are close to the original\n",
    "\n",
    "#### This visually explains:fewer dimensions = more compression = more information loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Lets see how this dimensionlity reduction affects classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "lr_original = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    solver=\"lbfgs\",\n",
    "    multi_class=\"auto\"\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "lr_original.fit(X_train, y_train)\n",
    "train_time_original = time.time() - start\n",
    "\n",
    "y_pred_orig = lr_original.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression on Original Features (64D)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_orig))\n",
    "print(\"Training time (sec):\", round(train_time_original, 3))\n",
    "print(classification_report(y_test, y_pred_orig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Note that data set is balanced from the support column and recall is also very good. So it is a balanced class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Now lets increase the PCA componets to 20 to compare the calssification fairness . Two would be very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_20 = PCA(n_components=20, random_state=42)\n",
    "\n",
    "X_train_pca = pca_20.fit_transform(X_train)\n",
    "X_test_pca = pca_20.transform(X_test)\n",
    "\n",
    "print(\"Original train shape:\", X_train.shape)\n",
    "print(\"PCA-reduced train shape:\", X_train_pca.shape)\n",
    "print(\"Variance retained:\", round(pca_20.explained_variance_ratio_.sum(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    solver=\"lbfgs\",\n",
    "    multi_class=\"auto\"\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "train_time_pca = time.time() - start\n",
    "\n",
    "y_pred_pca = lr_pca.predict(X_test_pca)\n",
    "\n",
    "print(\"Logistic Regression on PCA-reduced Features (20D)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_pca))\n",
    "print(\"Training time (sec):\", round(train_time_pca, 3))\n",
    "print(classification_report(y_test, y_pred_pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### This provides us the best advantage of PCA which is the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
