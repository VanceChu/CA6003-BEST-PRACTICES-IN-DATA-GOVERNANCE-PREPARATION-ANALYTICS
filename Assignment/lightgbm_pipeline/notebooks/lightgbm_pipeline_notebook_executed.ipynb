{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk - LightGBM Pipeline\n",
    "\n",
    "This notebook implements a complete machine learning pipeline for the Home Credit Default Risk competition.\n",
    "\n",
    "## Pipeline Stages\n",
    "1. **Data Loading** - Load 7 CSV tables\n",
    "2. **Preprocessing** - Encoding, outlier handling, missing value marking\n",
    "3. **Feature Engineering** - Derived features, aggregations, multi-table joins\n",
    "4. **Model Training** - K-Fold cross-validation with LightGBM\n",
    "5. **Output** - Predictions, feature importance, ROC curves, and training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 1: Data Loading\n",
    "# =============================================================================\n",
    "# Load all required libraries and read CSV files from disk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Configuration\n",
    "DEBUG = False  # Full training mode\n",
    "NUM_ROWS = 10000 if DEBUG else None\n",
    "DATA_PATH = './home-credit-default-risk/'  # Local data directory\n",
    "SUBMISSION_FILE = 'submission_kernel02.csv'\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    \"\"\"Context manager for timing code blocks.\"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"{title} - done in {time.time() - t0:.0f}s\")\n",
    "\n",
    "def sanitize_column_names(df):\n",
    "    \"\"\"Remove special characters from column names for LightGBM compatibility.\"\"\"\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# Load all datasets\n",
    "with timer(\"Loading all datasets\"):\n",
    "    application_train = pd.read_csv(f'{DATA_PATH}application_train.csv', nrows=NUM_ROWS)\n",
    "    application_test = pd.read_csv(f'{DATA_PATH}application_test.csv', nrows=NUM_ROWS)\n",
    "    bureau = pd.read_csv(f'{DATA_PATH}bureau.csv', nrows=NUM_ROWS)\n",
    "    bureau_balance = pd.read_csv(f'{DATA_PATH}bureau_balance.csv', nrows=NUM_ROWS)\n",
    "    previous_application = pd.read_csv(f'{DATA_PATH}previous_application.csv', nrows=NUM_ROWS)\n",
    "    pos_cash_balance = pd.read_csv(f'{DATA_PATH}POS_CASH_balance.csv', nrows=NUM_ROWS)\n",
    "    installments_payments = pd.read_csv(f'{DATA_PATH}installments_payments.csv', nrows=NUM_ROWS)\n",
    "    credit_card_balance = pd.read_csv(f'{DATA_PATH}credit_card_balance.csv', nrows=NUM_ROWS)\n",
    "\n",
    "print(f\"Train samples: {len(application_train)}, Test samples: {len(application_test)}\")\n",
    "print(f\"Bureau records: {len(bureau)}, Bureau balance records: {len(bureau_balance)}\")\n",
    "print(f\"Previous applications: {len(previous_application)}\")\n",
    "print(f\"POS cash balance: {len(pos_cash_balance)}\")\n",
    "print(f\"Installments payments: {len(installments_payments)}\")\n",
    "print(f\"Credit card balance: {len(credit_card_balance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 2: Data Preprocessing\n",
    "# =============================================================================\n",
    "# Handle encoding, outliers, and missing values\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    \"\"\"One-hot encoding for categorical columns with get_dummies.\"\"\"\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "with timer(\"Preprocessing application data\"):\n",
    "    df = pd.concat([application_train, application_test], ignore_index=True)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], _ = pd.factorize(df[bin_feature])\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "with timer(\"Preprocessing bureau and bureau_balance\"):\n",
    "    bureau_balance, bb_cat = one_hot_encoder(bureau_balance, nan_as_category=True)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category=True)\n",
    "\n",
    "with timer(\"Preprocessing previous applications\"):\n",
    "    previous_application, prev_cat = one_hot_encoder(previous_application, nan_as_category=True)\n",
    "    for col in ['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', \n",
    "                'DAYS_LAST_DUE', 'DAYS_TERMINATION']:\n",
    "        if col in previous_application.columns:\n",
    "            previous_application[col].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "with timer(\"Preprocessing other tables\"):\n",
    "    pos_cash_balance, pos_cat = one_hot_encoder(pos_cash_balance, nan_as_category=True)\n",
    "    installments_payments, ins_cat = one_hot_encoder(installments_payments, nan_as_category=True)\n",
    "    credit_card_balance, cc_cat = one_hot_encoder(credit_card_balance, nan_as_category=True)\n",
    "\n",
    "del application_train, application_test\n",
    "gc.collect()\n",
    "print(f\"Preprocessed main df shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 3: Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "with timer(\"Creating application features\"):\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "with timer(\"Creating bureau features\"):\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        if col in bureau_balance.columns:\n",
    "            bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    del bureau_balance, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    num_aggregations = {}\n",
    "    agg_mapping = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    for col, aggs in agg_mapping.items():\n",
    "        if col in bureau.columns:\n",
    "            num_aggregations[col] = aggs\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        if cat in bureau.columns:\n",
    "            cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat:\n",
    "        col_name = cat + \"_MEAN\"\n",
    "        if col_name in bureau.columns:\n",
    "            cat_aggregations[col_name] = ['mean']\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    if 'CREDIT_ACTIVE_Active' in bureau.columns:\n",
    "        active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "        if len(active) > 0:\n",
    "            active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "            active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "            bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "            del active_agg\n",
    "        del active\n",
    "    if 'CREDIT_ACTIVE_Closed' in bureau.columns:\n",
    "        closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "        if len(closed) > 0:\n",
    "            closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "            closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "            bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "            del closed_agg\n",
    "        del closed\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "    df = df.join(bureau_agg, how='left', on='SK_ID_CURR')\n",
    "    del bureau_agg\n",
    "    gc.collect()\n",
    "    print(f\"After bureau features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating previous application features\"):\n",
    "    previous_application['APP_CREDIT_PERC'] = previous_application['AMT_APPLICATION'] / previous_application['AMT_CREDIT']\n",
    "    prev_num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'], 'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'], 'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'], 'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'], 'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'], 'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    prev_num_aggregations = {k: v for k, v in prev_num_aggregations.items() if k in previous_application.columns}\n",
    "    cat_aggregations = {cat: ['mean'] for cat in prev_cat if cat in previous_application.columns}\n",
    "    prev_agg = previous_application.groupby('SK_ID_CURR').agg({**prev_num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    if 'NAME_CONTRACT_STATUS_Approved' in previous_application.columns:\n",
    "        approved = previous_application[previous_application['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "        if len(approved) > 0:\n",
    "            approved_agg = approved.groupby('SK_ID_CURR').agg(prev_num_aggregations)\n",
    "            approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "            prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "            del approved_agg\n",
    "        del approved\n",
    "    if 'NAME_CONTRACT_STATUS_Refused' in previous_application.columns:\n",
    "        refused = previous_application[previous_application['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "        if len(refused) > 0:\n",
    "            refused_agg = refused.groupby('SK_ID_CURR').agg(prev_num_aggregations)\n",
    "            refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "            prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "            del refused_agg\n",
    "        del refused\n",
    "    del previous_application\n",
    "    gc.collect()\n",
    "    df = df.join(prev_agg, how='left', on='SK_ID_CURR')\n",
    "    del prev_agg\n",
    "    gc.collect()\n",
    "    print(f\"After previous application features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating POS cash features\"):\n",
    "    aggregations = {'MONTHS_BALANCE': ['max', 'mean', 'size'], 'SK_DPD': ['max', 'mean'], 'SK_DPD_DEF': ['max', 'mean']}\n",
    "    aggregations = {k: v for k, v in aggregations.items() if k in pos_cash_balance.columns}\n",
    "    for cat in pos_cat:\n",
    "        if cat in pos_cash_balance.columns:\n",
    "            aggregations[cat] = ['mean']\n",
    "    pos_agg = pos_cash_balance.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    pos_agg['POS_COUNT'] = pos_cash_balance.groupby('SK_ID_CURR').size()\n",
    "    del pos_cash_balance\n",
    "    gc.collect()\n",
    "    df = df.join(pos_agg, how='left', on='SK_ID_CURR')\n",
    "    del pos_agg\n",
    "    gc.collect()\n",
    "    print(f\"After POS cash features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating installments features\"):\n",
    "    installments_payments['PAYMENT_PERC'] = installments_payments['AMT_PAYMENT'] / installments_payments['AMT_INSTALMENT']\n",
    "    installments_payments['PAYMENT_DIFF'] = installments_payments['AMT_INSTALMENT'] - installments_payments['AMT_PAYMENT']\n",
    "    installments_payments['DPD'] = installments_payments['DAYS_ENTRY_PAYMENT'] - installments_payments['DAYS_INSTALMENT']\n",
    "    installments_payments['DBD'] = installments_payments['DAYS_INSTALMENT'] - installments_payments['DAYS_ENTRY_PAYMENT']\n",
    "    installments_payments['DPD'] = installments_payments['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    installments_payments['DBD'] = installments_payments['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins_aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'], 'DPD': ['max', 'mean', 'sum'], 'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'], 'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'], 'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    ins_aggregations = {k: v for k, v in ins_aggregations.items() if k in installments_payments.columns}\n",
    "    for cat in ins_cat:\n",
    "        if cat in installments_payments.columns:\n",
    "            ins_aggregations[cat] = ['mean']\n",
    "    ins_agg = installments_payments.groupby('SK_ID_CURR').agg(ins_aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    ins_agg['INSTAL_COUNT'] = installments_payments.groupby('SK_ID_CURR').size()\n",
    "    del installments_payments\n",
    "    gc.collect()\n",
    "    df = df.join(ins_agg, how='left', on='SK_ID_CURR')\n",
    "    del ins_agg\n",
    "    gc.collect()\n",
    "    print(f\"After installments features: {df.shape}\")\n",
    "\n",
    "with timer(\"Creating credit card features\"):\n",
    "    if 'SK_ID_PREV' in credit_card_balance.columns:\n",
    "        credit_card_balance.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    numeric_cols = credit_card_balance.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != 'SK_ID_CURR']\n",
    "    cc_agg = credit_card_balance.groupby('SK_ID_CURR')[numeric_cols].agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    cc_agg['CC_COUNT'] = credit_card_balance.groupby('SK_ID_CURR').size()\n",
    "    del credit_card_balance\n",
    "    gc.collect()\n",
    "    df = df.join(cc_agg, how='left', on='SK_ID_CURR')\n",
    "    del cc_agg\n",
    "    gc.collect()\n",
    "\n",
    "df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "print(f\"Final df shape after all features: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 4: Model Training\n",
    "# =============================================================================\n",
    "# 10-Fold cross-validation with LightGBM\n",
    "# Returns per-fold predictions for individual ROC curves\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "def kfold_lightgbm(df, num_folds, stratified=False):\n",
    "    \"\"\"Train LightGBM with K-Fold CV. Returns per-fold data for visualization.\"\"\"\n",
    "    train_df = df[df['TARGET'].notnull()].copy()\n",
    "    test_df = df[df['TARGET'].isnull()].copy()\n",
    "    print(f\"Starting LightGBM. Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    \n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    training_history = []\n",
    "    fold_results = []  # Store per-fold predictions and labels for ROC curves\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']]\n",
    "    print(f\"Number of features: {len(feats)}\")\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "        \n",
    "        clf = LGBMClassifier(\n",
    "            n_jobs=4, n_estimators=10000, learning_rate=0.02, num_leaves=34,\n",
    "            colsample_bytree=0.9497036, subsample=0.8715623, max_depth=8,\n",
    "            reg_alpha=0.041545473, reg_lambda=0.0735294, min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775, verbose=-1,\n",
    "        )\n",
    "        \n",
    "        eval_results = {}\n",
    "        clf.fit(\n",
    "            train_x, train_y,\n",
    "            eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "            eval_names=['train', 'valid'],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=200),\n",
    "                lgb.log_evaluation(period=200),\n",
    "                lgb.record_evaluation(eval_results)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        training_history.append({\n",
    "            'fold': n_fold + 1,\n",
    "            'train_auc': eval_results['train']['auc'],\n",
    "            'valid_auc': eval_results['valid']['auc'],\n",
    "            'best_iteration': clf.best_iteration_\n",
    "        })\n",
    "        \n",
    "        valid_preds = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "        \n",
    "        # Store per-fold results for individual ROC curves\n",
    "        fold_results.append({\n",
    "            'fold': n_fold + 1,\n",
    "            'y_true': valid_y.values,\n",
    "            'y_pred': valid_preds,\n",
    "            'auc': roc_auc_score(valid_y, valid_preds)\n",
    "        })\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        print(f'Fold {n_fold + 1:2d} AUC : {fold_results[-1][\"auc\"]:.6f}')\n",
    "        \n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    \n",
    "    full_auc = roc_auc_score(train_df['TARGET'], oof_preds)\n",
    "    print(f'Full AUC score: {full_auc:.6f}')\n",
    "    \n",
    "    test_df['TARGET'] = sub_preds\n",
    "    \n",
    "    return (test_df[['SK_ID_CURR', 'TARGET']], feature_importance_df, \n",
    "            oof_preds, train_df['TARGET'].values, training_history, fold_results)\n",
    "\n",
    "num_folds = 3 if DEBUG else 10\n",
    "with timer(f\"Training LightGBM with {num_folds}-Fold\"):\n",
    "    submission_df, feature_importance_df, oof_preds, train_targets, training_history, fold_results = kfold_lightgbm(\n",
    "        df, num_folds=num_folds, stratified=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 5: Output & Visualization\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Feature Importance ---\n",
    "def display_importances(feature_importance_df):\n",
    "    cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "            .groupby(\"feature\").mean()\n",
    "            .sort_values(by=\"importance\", ascending=False)[:40].index)\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", \n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False), palette=\"viridis\")\n",
    "    plt.title('LightGBM Feature Importance (Average over Folds)', fontsize=14)\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png', dpi=150)\n",
    "    plt.show()\n",
    "    return cols\n",
    "\n",
    "# --- 2. Per-Fold ROC Curves ---\n",
    "def plot_per_fold_roc_curves(fold_results):\n",
    "    \"\"\"Plot individual ROC curve for each fold.\"\"\"\n",
    "    n_folds = len(fold_results)\n",
    "    cols = min(5, n_folds)  # 5 columns max\n",
    "    rows = (n_folds + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    axes = axes.flatten() if n_folds > 1 else [axes]\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.9, n_folds))\n",
    "    \n",
    "    for idx, fold_data in enumerate(fold_results):\n",
    "        ax = axes[idx]\n",
    "        fpr, tpr, _ = roc_curve(fold_data['y_true'], fold_data['y_pred'])\n",
    "        auc = fold_data['auc']\n",
    "        \n",
    "        ax.plot(fpr, tpr, color=colors[idx], lw=2, label=f'AUC = {auc:.4f}')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1.05])\n",
    "        ax.set_xlabel('FPR', fontsize=10)\n",
    "        ax.set_ylabel('TPR', fontsize=10)\n",
    "        ax.set_title(f'Fold {fold_data[\"fold\"]}', fontsize=12)\n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_folds, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('ROC Curves for Each Fold', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curves_per_fold.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. Combined ROC Curve ---\n",
    "def plot_combined_roc_curve(y_true, y_pred, fold_results):\n",
    "    \"\"\"Plot combined OOF ROC curve with all folds overlay.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot each fold in light color\n",
    "    for fold_data in fold_results:\n",
    "        fpr, tpr, _ = roc_curve(fold_data['y_true'], fold_data['y_pred'])\n",
    "        plt.plot(fpr, tpr, alpha=0.3, lw=1)\n",
    "    \n",
    "    # Plot combined OOF\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Mean OOF ROC (AUC = {auc_score:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'navy', lw=2, linestyle='--', label='Random')\n",
    "    \n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Combined ROC Curve (All Folds + OOF)', fontsize=14)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curve.png', dpi=150)\n",
    "    plt.show()\n",
    "    return auc_score\n",
    "\n",
    "# --- 4. Training Curves ---\n",
    "def plot_training_curves(training_history):\n",
    "    n_folds = len(training_history)\n",
    "    cols = min(5, n_folds)\n",
    "    rows = (n_folds + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n",
    "    axes = axes.flatten() if n_folds > 1 else [axes]\n",
    "    \n",
    "    for idx, fold_history in enumerate(training_history):\n",
    "        ax = axes[idx]\n",
    "        iterations = range(1, len(fold_history['train_auc']) + 1)\n",
    "        ax.plot(iterations, fold_history['train_auc'], label='Train', color='blue', alpha=0.7)\n",
    "        ax.plot(iterations, fold_history['valid_auc'], label='Valid', color='red', alpha=0.7)\n",
    "        ax.axvline(x=fold_history['best_iteration'], color='green', linestyle='--', alpha=0.7)\n",
    "        ax.set_xlabel('Iterations', fontsize=9)\n",
    "        ax.set_ylabel('AUC', fontsize=9)\n",
    "        ax.set_title(f'Fold {fold_history[\"fold\"]} (best: {fold_history[\"best_iteration\"]})', fontsize=10)\n",
    "        ax.legend(loc='lower right', fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    for idx in range(n_folds, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Training Curves (AUC vs Iterations)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# === Execute Visualizations ===\n",
    "\n",
    "with timer(\"Saving submission file\"):\n",
    "    submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "    print(f\"Submission saved to: {SUBMISSION_FILE}\")\n",
    "    print(f\"Submission shape: {submission_df.shape}\")\n",
    "\n",
    "with timer(\"Generating feature importance\"):\n",
    "    top_features = display_importances(feature_importance_df)\n",
    "\n",
    "with timer(\"Generating per-fold ROC curves\"):\n",
    "    plot_per_fold_roc_curves(fold_results)\n",
    "\n",
    "with timer(\"Generating combined ROC curve\"):\n",
    "    oof_auc = plot_combined_roc_curve(train_targets, oof_preds, fold_results)\n",
    "\n",
    "with timer(\"Generating training curves\"):\n",
    "    plot_training_curves(training_history)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of folds: {len(fold_results)}\")\n",
    "print(f\"Features used: {len(feature_importance_df['feature'].unique())}\")\n",
    "print(f\"\\nPer-Fold AUC:\")\n",
    "for fr in fold_results:\n",
    "    print(f\"  Fold {fr['fold']:2d}: {fr['auc']:.6f}\")\n",
    "print(f\"\\nMean Fold AUC: {np.mean([fr['auc'] for fr in fold_results]):.6f}\")\n",
    "print(f\"OOF AUC:       {oof_auc:.6f}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {SUBMISSION_FILE}\")\n",
    "print(f\"  - lgbm_importances01.png\")\n",
    "print(f\"  - roc_curves_per_fold.png\")\n",
    "print(f\"  - roc_curve.png\")\n",
    "print(f\"  - training_curves.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
